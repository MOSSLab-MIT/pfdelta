{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07a0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing, TAGConv\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eaefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm: \n",
    "# Build the train-test split dicts for the task \n",
    "# Specify the task name to do this \n",
    "# In the main function -- check if the directory exists \n",
    "    # directory -- in the main dataset class, work with the case name\n",
    "    # inside each case name, create -- processed_task_%_%\n",
    "        # inside have a train, val, test split\n",
    "    # then in the process function for the main dataset class, check if this folder exists, if it doesn't then create it\n",
    "    # return the processed folders of the ones you need (split up just feasible and just feasible / around the nose?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdf53df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEASIBILITY_CONFIG = {\n",
    "    \"just feasible\": {\n",
    "        \"N\": 56000,\n",
    "        \"N_1\": 29000,\n",
    "        \"N_2\": 20000,\n",
    "        \"test\": {\"N\": 2000, \"N_1\": 2000, \"N_2\": 2000}\n",
    "    },\n",
    "    \"around the nose\": {\n",
    "        \"N\": 7200,\n",
    "        \"N_1\": 7200,\n",
    "        \"N_2\": 7200,\n",
    "        \"test\": None  # no test set for this regime\n",
    "    },\n",
    "    \"just the nose\": {\n",
    "        \"N\": 2000,\n",
    "        \"N_1\": 2000,\n",
    "        \"N_2\": 2000,\n",
    "        \"test\": {\"N\": 200, \"N_1\": 200, \"N_2\": 200}\n",
    "    },\n",
    "}\n",
    "\n",
    "TASK_CONFIG = {\n",
    "    1.1: {\"N\": 54000, \"N_1\": 0, \"N_2\": 0},\n",
    "    1.2: {\"N\": 27000, \"N_1\": 27000, \"N_2\": 0},\n",
    "    1.3: {\"N\": 18000, \"N_1\": 18000, \"N_2\": 18000},\n",
    "    2.1: {\"N\": 18000, \"N_1\": 18000, \"N_2\": 18000},\n",
    "    2.2: {\"N\": 12000, \"N_1\": 12000, \"N_2\": 12000},\n",
    "    2.3: {\"N\": 6000,  \"N_1\": 6000,  \"N_2\": 6000},\n",
    "    3.1: {\"N\": 18000, \"N_1\": 18000, \"N_2\": 18000},\n",
    "    3.2: {\"N\": 18000, \"N_1\": 18000, \"N_2\": 18000},\n",
    "    3.3: {\"N\": 18000, \"N_1\": 18000, \"N_2\": 18000}, # can add 4.1 and 4.2 later? \n",
    "}\n",
    "\n",
    "\n",
    "def build_train_test_mapping(task, seed=11, feasibility=\"just feasible\"):\n",
    "\n",
    "    if feasibility not in FEASIBILITY_CONFIG:\n",
    "        raise AttributeError(f\"Unknown feasibility setting: {feasibility}\")\n",
    "    if task not in TASK_CONFIG:\n",
    "        raise AttributeError(f\"Unknown task: {task}\")\n",
    "\n",
    "    feasibility_config = FEASIBILITY_CONFIG[feasibility]\n",
    "    task_config = TASK_CONFIG[task]\n",
    "    mappings = {}\n",
    "\n",
    "    for grid_type in [\"N\", \"N_1\", \"N_2\"]:\n",
    "        num_samples = feasibility_config[grid_type]\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        test_cfg = feasibility_config.get(\"test\", {})\n",
    "        test_size = test_cfg.get(grid_type) if test_cfg else 0\n",
    "\n",
    "        if test_size:\n",
    "            test_indices = indices[:test_size]\n",
    "            train_indices = indices[test_size:]\n",
    "            mappings[f\"{grid_type}_test_mapping\"] = {int(i): int(j) for i, j in enumerate(test_indices)}\n",
    "        else:\n",
    "            train_indices = indices\n",
    "\n",
    "        train_size = task_config[grid_type] if task_config[grid_type] else None\n",
    "        if train_size:\n",
    "            mappings[f\"{grid_type}_train_mapping\"] = {int(i): int(j) for i, j in zip(range(train_size), train_indices[:train_size])}\n",
    "\n",
    "    return mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d77cae35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['N_test_mapping', 'N_train_mapping', 'N_1_test_mapping', 'N_1_train_mapping', 'N_2_test_mapping', 'N_2_train_mapping'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_train_test_mapping(2.1, seed=11, feasibility=\"just the nose\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f64978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_train_test_mapping(task, seed=11, feasibility=\"just feasible\"):\n",
    "\n",
    "    # depending on feasibility, we change how many samples we shuffle / sample \n",
    "    if feasibility == \"just feasible\": \n",
    "        N_indices, N_1_indices, N_2_indices = 56000, 29000, 20000\n",
    "        N_test_num, N_1_test_num, N_2_test_num = 2000\n",
    "    elif feasibility == \"around the nose\": \n",
    "        N_indices = N_1_indices = N_2_indices = 7200\n",
    "        N_test_num, N_1_test_num, N_2_test_num = None # We never test on this subset\n",
    "    elif feasibility == \"just the nose\": \n",
    "        N_indices = N_1_indices = N_2_indices = 2000\n",
    "        N_test_num, N_1_test_num, N_2_test_num = 200\n",
    "\n",
    "    # Shuffle the total indices to generate mappings\n",
    "    np.random.seed(seed)\n",
    "    N_total_indices = np.arange(N_indices)\n",
    "    np.random.shuffle(N_total_indices)\n",
    "    N_1_total_indices = np.arange(N_1_indices)\n",
    "    np.random.shuffle(N_1_total_indices)\n",
    "    N_2_total_indices = np.arange(N_2_indices)\n",
    "    np.random.shuffle(N_2_total_indices)\n",
    "\n",
    "    # generate test mappings only for just feasible and just the nose \n",
    "    if feasibility == \"just feasible\" or \"just the nose\": \n",
    "        N_test_mapping = {int(i): int(j) for i, j in zip(np.arange(N_test_num), N_total_indices[:N_test_num])}\n",
    "        N_1_test_mapping = {int(i): int(j) for i, j in zip(np.arange(N_1_test_num), N_1_total_indices[:N_1_test_num])}\n",
    "        N_2_test_mapping = {int(i): int(j) for i, j in zip(np.arange(N_2_test_num), N_2_total_indices[:N_2_test_num])}\n",
    "\n",
    "    if task == 1.1: \n",
    "        # train on N, test on N, N-1, N-2 \n",
    "        # build train set mapping \n",
    "        N_train_num = N_indices - N_test_num\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(54000), N_test_indices[2000:])}\n",
    "\n",
    "    elif task == 1.2: \n",
    "        # train on N, N-1 \n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(27000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(27000), N_1_test_indices[2000:])}\n",
    "\n",
    "    elif task == 1.3: \n",
    "        # train on N, N-1, N-2\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 2.1: \n",
    "        # train on N, N-1, N-2, low data efficiency\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 2.2: \n",
    "        # train on N, N-1, N-2, med data efficiency\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(12000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(12000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(12000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 2.3: \n",
    "        # train on N, N-1, N-2, high data efficiency\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(6000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(6000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(6000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 3.1: \n",
    "        # train on N, N-1, N-2\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 3.2: \n",
    "        # train on N, N-1, N-2\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 3.3: \n",
    "        # train on N, N-1, N-2\n",
    "        N_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_test_indices[2000:])}\n",
    "        N_1_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_1_test_indices[2000:])}\n",
    "        N_2_train_mapping = {int(i): int(j) for i, j in zip(np.arange(18000), N_2_test_indices[2000:])}\n",
    "\n",
    "    elif task == 4.1: \n",
    "        pass\n",
    "    elif task == 4.2: \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc12b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', add_bus_type=False, transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.case_name = case_name\n",
    "        self.force_reload = force_reload\n",
    "        self.add_bus_type = add_bus_type\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()]) \n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2, 'all': 3}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt', 'all.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        # Bus nodes\n",
    "        pf_x, pf_y = [], []\n",
    "        bus_voltages = []\n",
    "        bus_type = []\n",
    "        bus_shunts = []\n",
    "        bus_gen, bus_demand = [], []\n",
    "\n",
    "        PQ_bus_x, PQ_bus_y = [], []\n",
    "        PV_bus_x, PV_bus_y = [], []\n",
    "        PV_demand, PV_generation = [], []\n",
    "        slack_x, slack_y = [], []\n",
    "        slack_demand, slack_generation = [], []\n",
    "        PV_to_bus, PQ_to_bus, slack_to_bus = [], [], []\n",
    "        pq_idx, pv_idx, slack_idx = 0, 0, 0\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "            \n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            bus_voltages.append(torch.tensor([va, vm]))\n",
    "\n",
    "            # Shunts \n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_shunts.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            bus_demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "                if int(gen['gen_bus']) == bus_id: \n",
    "                    if gen['gen_status'] == 1:\n",
    "                        gen_sol = solution_data['gen'][gen_id]\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            bus_gen.append(torch.tensor([pg, qg]))\n",
    "\n",
    "            # Now decide final bus type\n",
    "            bus_type_now = bus['bus_type']\n",
    "\n",
    "            if bus_type_now == 2 and pg == 0.0 and qg == 0.0:\n",
    "                bus_type_now = 1  # PV bus with no gen --> becomes PQ\n",
    "                # maybe add an assert here to check if all gens were off.\n",
    "\n",
    "            bus_type.append(torch.tensor(bus_type_now))\n",
    "\n",
    "            if bus_type_now == 1:\n",
    "                pf_x.append(torch.tensor([pd, qd]))\n",
    "                pf_y.append(torch.tensor([va, vm]))\n",
    "\n",
    "                PQ_bus_x.append(torch.tensor([pd, qd]))\n",
    "                PQ_bus_y.append(torch.tensor([va, vm]))\n",
    "                PQ_to_bus.append(torch.tensor([pq_idx, bus_idx]))\n",
    "                pq_idx += 1\n",
    "            elif bus_type_now == 2:\n",
    "                pf_x.append(torch.tensor([pg - pd, vm]))\n",
    "                pf_y.append(torch.tensor([qg - qd, va]))\n",
    "                \n",
    "                PV_bus_x.append(torch.tensor([pg - pd, vm]))\n",
    "                PV_bus_y.append(torch.tensor([qg - qd, va]))\n",
    "                PV_demand.append(torch.tensor([pd, qd]))\n",
    "                PV_generation.append(torch.tensor([pg, qg]))\n",
    "                PV_to_bus.append(torch.tensor([pv_idx, bus_idx]))\n",
    "                pv_idx += 1\n",
    "            elif bus_type_now == 3:\n",
    "                pf_x.append(torch.tensor([va, vm]))\n",
    "                pf_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "\n",
    "                slack_x.append(torch.tensor([va, vm]))\n",
    "                slack_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                slack_demand.append(torch.tensor([pd, qd]))\n",
    "                slack_generation.append(torch.tensor([pg, qg]))\n",
    "                slack_to_bus.append(torch.tensor([slack_idx, bus_idx]))\n",
    "                slack_idx += 1\n",
    "\n",
    "        generation, limits, slack_gen  = [], [], []\n",
    "\n",
    "        # Generator nodes   \n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_sol = solution_data['gen'][gen_id] \n",
    "                pmin, pmax, qmin, qmax = gen['pmin'], gen['pmax'], gen['qmin'], gen['qmax']\n",
    "                pgen, qgen = gen_sol['pg'], gen_sol['qg']\n",
    "                limits.append(torch.tensor([pmin, pmax, qmin, qmax]))\n",
    "                generation.append(torch.tensor([pgen, qgen]))\n",
    "                is_slack = torch.tensor(\n",
    "                        1 if network_data['bus'][str(gen['gen_bus'])]['bus_type'] == 3 else 0,\n",
    "                        dtype=torch.bool\n",
    "                                )\n",
    "                slack_gen.append(is_slack)\n",
    "            else:\n",
    "                assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "        # Load nodes\n",
    "        demand = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            pd, qd = load['pd'], load['qd']\n",
    "            demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "        # Edges\n",
    "        # bus to bus edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            if branch['br_status'] == 0:\n",
    "                continue  # Skip inactive branches\n",
    "\n",
    "            from_bus = int(branch['f_bus']) - 1 \n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'], \n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            assert branch_sol is not None, f\"Missing solution for active branch {branch_id_str}\"\n",
    "\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "\n",
    "        # bus to gen edges\n",
    "        gen_to_bus_index = []\n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_bus = torch.tensor(gen['gen_bus']) - 1\n",
    "                gen_to_bus_index.append(torch.tensor([int(gen_id) - 1, gen_bus]))\n",
    "\n",
    "        # bus to load edges\n",
    "        load_to_bus_index = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            load_bus = torch.tensor(load['load_bus']) - 1\n",
    "            load_to_bus_index.append(torch.tensor([int(load_id) - 1, load_bus]))\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['bus'].x = torch.stack(pf_x)\n",
    "        data['bus'].y = torch.stack(pf_y)\n",
    "        data['bus'].bus_gen = torch.stack(bus_gen) # aggregated\n",
    "        data['bus'].bus_demand = torch.stack(bus_demand) # aggregated\n",
    "        data['bus'].bus_voltages = torch.stack(bus_voltages)\n",
    "        data['bus'].bus_type = torch.stack(bus_type)\n",
    "        data['bus'].shunt = torch.stack(bus_shunts)\n",
    "\n",
    "        data['gen'].limits = torch.stack(limits)\n",
    "        data['gen'].generation = torch.stack(generation)\n",
    "        data['gen'].slack_gen = torch.stack(slack_gen)\n",
    "\n",
    "        data['load'].demand = torch.stack(demand)\n",
    "\n",
    "        if self.add_bus_type:\n",
    "            data['PQ'].x = torch.stack(PQ_bus_x) \n",
    "            data['PQ'].y = torch.stack(PQ_bus_y)\n",
    "\n",
    "            data['PV'].x = torch.stack(PV_bus_x) \n",
    "            data['PV'].y = torch.stack(PV_bus_y) \n",
    "            data['PV'].generation = torch.stack(PV_generation) \n",
    "            data['PV'].demand = torch.stack(PV_demand) \n",
    "\n",
    "            data['slack'].x = torch.stack(slack_x) \n",
    "            data['slack'].y = torch.stack(slack_y)\n",
    "            data['slack'].generation = torch.stack(slack_generation) \n",
    "            data['slack'].demand = torch.stack(slack_demand)         \n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('bus', 'branch', 'bus'): edge_index,\n",
    "            ('gen', 'gen_link', 'bus'): gen_to_bus_index,\n",
    "            ('load', 'load_link', 'bus'): load_to_bus_index\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1) \n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "            if link_name == ('bus', 'branch', 'bus'): \n",
    "                data[link_name].edge_attr = torch.stack(edge_attr) \n",
    "        \n",
    "        if self.add_bus_type:\n",
    "            for link_name, edges in {\n",
    "                ('PV', 'PV_link', 'bus'): PV_to_bus,\n",
    "                ('PQ', 'PQ_link', 'bus'): PQ_to_bus,\n",
    "                ('slack', 'slack_link', 'bus'): slack_to_bus\n",
    "            }.items():\n",
    "                edge_tensor = torch.stack(edges, dim=1) \n",
    "                data[link_name].edge_index = edge_tensor\n",
    "                data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):],\n",
    "            'all': fnames  # 👈 this uses all samples\n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaPFNet(PFDeltaDataset): \n",
    "    def __init__(self, root_dir='data', case_name='', split='train', add_bus_type=False, transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__(root_dir, case_name, split, add_bus_type, transform, pre_transform, pre_filter, force_reload)\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        # call base version\n",
    "        data = super().build_heterodata(pm_case)\n",
    "\n",
    "        num_buses = data['bus'].x.size(0)\n",
    "        bus_types = data['bus'].bus_type\n",
    "        pf_x = data['bus'].x\n",
    "        pf_y = data['bus'].y\n",
    "        shunts = data['bus'].shunt\n",
    "        num_gens = data['gen'].generation.size(0)\n",
    "        num_loads = data['load'].demand.size(0)\n",
    "\n",
    "        # New node features for PFNet\n",
    "        x_pfnet = []\n",
    "        y_pfnet = []\n",
    "        for i in range(num_buses):\n",
    "            bus_type = int(bus_types[i].item())\n",
    "\n",
    "            # One-hot encode bus type\n",
    "            one_hot = torch.zeros(4) \n",
    "            one_hot[bus_type - 1] = 1  \n",
    "            gs, bs = shunts[i]\n",
    "\n",
    "            # Prediction mask\n",
    "            if bus_type == 1:   # PQ\n",
    "                pred_mask = torch.tensor([1, 1, 0, 0, 0, 0])\n",
    "                va, vm =  pf_y[i] \n",
    "                pd, qd = pf_x[i]\n",
    "                input_mask = (1 - pred_mask).float()\n",
    "                input_feats = torch.tensor([vm, va, pd, qd, gs, bs]) * input_mask\n",
    "                features = torch.cat([one_hot, input_feats])\n",
    "                y = torch.tensor([vm, va, pd, qd, gs, bs])\n",
    "            elif bus_type == 2: # PV\n",
    "                pred_mask = torch.tensor([0, 1, 0, 1, 0, 0])\n",
    "                pg_pd, vm =  pf_x[i]\n",
    "                qg_qd, va = pf_y[i]\n",
    "                input_mask = (1 - pred_mask).float()\n",
    "                input_feats = torch.tensor([vm, va, pg_pd, qg_qd, gs, bs]) * input_mask\n",
    "                features = torch.cat([one_hot, input_feats])\n",
    "                y = torch.tensor([vm, va, pg_pd, qg_qd, gs, bs])\n",
    "            elif bus_type == 3: # Slack\n",
    "                pred_mask = torch.tensor([0, 0, 1, 1, 0, 0])\n",
    "                va, vm =  pf_x[i]\n",
    "                pg_pd, qg_qd = pf_y[i]\n",
    "                input_mask = (1 - pred_mask).float()\n",
    "                input_feats = torch.tensor([vm, va, pg_pd, qg_qd, gs, bs]) * input_mask\n",
    "                features = torch.cat([one_hot, input_feats])\n",
    "                y = torch.tensor([vm, va, pg_pd, qg_qd, gs, bs])\n",
    "\n",
    "            x_pfnet.append(torch.cat([features, pred_mask]))\n",
    "            y_pfnet.append(y)\n",
    "\n",
    "        x_pfnet = torch.stack(x_pfnet)  # shape [N, 4+6+6=16]\n",
    "        y_pfnet = torch.stack(y_pfnet)  # shape [N, 6]\n",
    "\n",
    "        if self.split == 'train':\n",
    "            # Strip one-hot and pred_mask\n",
    "            x_cont = x_pfnet[:, 4:10]  # shape [N, 6]\n",
    "            y_cont = y_pfnet           # shape [N, 6]\n",
    "\n",
    "            xy = torch.cat([x_cont, y_cont], dim=0)\n",
    "            mean = xy.mean(dim=0, keepdim=True)\n",
    "            std = xy.std(dim=0, keepdim=True) + 1e-7\n",
    "\n",
    "            self.norm_mean = mean\n",
    "            self.norm_std = std\n",
    "\n",
    "            x_cont_norm = (x_cont - mean) / std\n",
    "            y_norm = (y_cont - mean) / std\n",
    "\n",
    "            x_normalized = torch.cat([x_pfnet[:, :4], x_cont_norm, x_pfnet[:, 10:]], dim=1)\n",
    "\n",
    "            data['bus'].x = x_normalized\n",
    "            data['bus'].y = y_norm\n",
    "        else:\n",
    "            data['bus'].x = x_pfnet\n",
    "            data['bus'].y = y_pfnet\n",
    "\n",
    "        data['gen'].num_nodes = num_gens\n",
    "        data['load'].num_nodes = num_loads\n",
    "\n",
    "        edge_attrs = []\n",
    "        for attr in data['bus', 'branch', 'bus'].edge_attr:\n",
    "            r, x = attr[0], attr[1]\n",
    "            b = attr[2] + attr[4]\n",
    "            tau, angle = attr[6], attr[7]\n",
    "            edge_attrs.append(torch.tensor([r, x, b, tau, angle]))\n",
    "\n",
    "        data['bus', 'branch', 'bus'].edge_attr = torch.stack(edge_attrs)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f242385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train (8050 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 8050/8050 [01:57<00:00, 68.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: val (1006 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 1006/1006 [00:11<00:00, 87.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test (1007 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 1007/1007 [00:10<00:00, 98.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: all (10063 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building all data: 100%|██████████| 10063/10063 [01:50<00:00, 91.06it/s] \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "case_14_data = PFDeltaPFNet(root_dir='data/gns_data/case14_n/', case_name='case14', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_14_data = PFDeltaPFNet(root_dir='data/gns_data/case14_n/', case_name='case14', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the architecture works / train with supervised loss\n",
    "# integrate back into main; now all the architectures should be on there\n",
    "# then focus on writing the introduction / related work / preliminaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d95b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeAggregation(MessagePassing):\n",
    "    \"\"\"\n",
    "    Custom MessagePassing module for aggregating edge features\n",
    "    to compute node-level representations.\n",
    "    Params:\n",
    "        nfeature_dim (int): Dimensionality of node features.\n",
    "        efeature_dim (int): Dimensionality of edge features.\n",
    "        hidden_dim (int): Hidden dimension of the MLP.\n",
    "        output_dim (int): Dimensionality of the output node features.\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeature_dim, efeature_dim, hidden_dim, output_dim):\n",
    "        super().__init__(aggr='add')\n",
    "        self.nfeature_dim = nfeature_dim\n",
    "        self.efeature_dim = efeature_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.edge_aggr = nn.Sequential(\n",
    "            nn.Linear(nfeature_dim*2 + efeature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        '''\n",
    "        Compute messages passed from source to target nodes in the graph.\n",
    "        Params:\n",
    "            x_i (torch.Tensor): Target node features (num_edges, nfeature_dim).\n",
    "            x_j (torch.Tensor): Source node features (num_edges, nfeature_dim).\n",
    "            edge_attr (torch.Tensor): Edge features (num_edges, efeature_dim).\n",
    "        Returns:\n",
    "            (torch.Tensor): Aggregated features for each edge (num_edges, output_dim).\n",
    "        '''\n",
    "        return self.edge_aggr(torch.cat([x_i, x_j, edge_attr], dim=-1)) # PNAConv style\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass for aggregating edge features and computing node embeddings.\n",
    "        N is the batch size.\n",
    "        Params:\n",
    "            x (torch.Tensor): Node features (N, num_nodes, nfeature_dim).\n",
    "            edge_index (torch.Tensor): Graph connectivity in COO format (N, 2, num_edges).\n",
    "            edge_attr (torch.Tensor): Edge features (N, num_edges, efeature_dim).\n",
    "        Returns:\n",
    "            torch.Tensor: Node embeddings after aggregating edge features\n",
    "                        (N, num_nodes, output_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # no self loop because NO EDGE ATTR FOR SELF LOOP\n",
    "\n",
    "        # Step 2: Calculate the degree of each node.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0.\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] \n",
    "\n",
    "        # Step 3: Feature transformation.\n",
    "        # x = self.linear(x) # no feature transformation\n",
    "\n",
    "        # Step 4: Propagation\n",
    "        out = self.propagate(x=x, edge_index=edge_index, edge_attr=edge_attr, norm=norm)\n",
    "\n",
    "        return out\n",
    "\n",
    "class PowerFlowNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PowerFlowNet: A Graph Neural Network for power flow approximation in graphs.\n",
    "    Model combines message passing and convolutions to predict node-level\n",
    "    outputs (e.g., voltages, angles) in power systems:\n",
    "    - Mask embedding for selective feature predictions.\n",
    "    - Multi-step message passing layers combined with convolution layers.\n",
    "    Params:\n",
    "        nfeature_dim (int): Dimensionality of node features.\n",
    "        efeature_dim (int): Dimensionality of edge features.\n",
    "        output_dim (int): Dimensionality of the output node embeddings.\n",
    "        hidden_dim (int): Hidden layer dimensionality.\n",
    "        n_gnn_layers (int): Number of GNN layers in the network.\n",
    "        K (int): Number of hops for the TAGConv layer.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeature_dim, efeature_dim, output_dim, hidden_dim, n_gnn_layers, K, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.nfeature_dim = nfeature_dim\n",
    "        self.efeature_dim = efeature_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_gnn_layers = n_gnn_layers\n",
    "        self.K = K\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.mask_embd = nn.Sequential(\n",
    "                nn.Linear(nfeature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, nfeature_dim)\n",
    "        )\n",
    "\n",
    "        if n_gnn_layers == 1:\n",
    "            self.layers.append(EdgeAggregation(nfeature_dim, efeature_dim, hidden_dim, hidden_dim))\n",
    "            self.layers.append(TAGConv(hidden_dim, output_dim, K=K))\n",
    "        else:\n",
    "            self.layers.append(EdgeAggregation(nfeature_dim, efeature_dim, hidden_dim, hidden_dim))\n",
    "            self.layers.append(TAGConv(hidden_dim, hidden_dim, K=K))\n",
    "\n",
    "        for _ in range(n_gnn_layers-2):\n",
    "            self.layers.append(EdgeAggregation(hidden_dim, efeature_dim, hidden_dim, hidden_dim))\n",
    "            self.layers.append(TAGConv(hidden_dim, hidden_dim, K=K))\n",
    "\n",
    "        # NO SLACK BUS OPERATIONS INCLUDED\n",
    "        # self.layers.append(TAGConv(hidden_dim, output_dim, K=K))\n",
    "        # self.slack_aggr = SlackAggregation(hidden_dim, hidden_dim, 'to_slack')\n",
    "        # self.slack_propagate = SlackAggregation(hidden_dim, hidden_dim, 'from_slack')\n",
    "        self.layers.append(EdgeAggregation(hidden_dim, efeature_dim, hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate, inplace=False)\n",
    "\n",
    "    def is_directed(self, edge_index):\n",
    "        \"\"\"\n",
    "        Determines if a graph is directed by examining the first edge.\n",
    "        Params:\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, num_edges).\n",
    "        Returns:\n",
    "            (bool): True if the graph is directed, False otherwise.\n",
    "        \"\"\"\n",
    "        if edge_index.shape[1] == 0:\n",
    "            # no edge at all, only single nodes. automatically undirected\n",
    "            return False\n",
    "        # if there is the reverse of the first edge does not exist, then directed.\n",
    "        return edge_index[0, 0] not in edge_index[1, edge_index[0, :] == edge_index[1, 0]]\n",
    "\n",
    "    def undirect_graph(self, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Converts a directed graph into an undirected one by duplicating edges.\n",
    "        Params:\n",
    "            edge_index (torch.Tensor): Edge indices (2, num_edges).\n",
    "            edge_attr (torch.Tensor): Edge features (num_edges, efeature_dim).\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Updated edge indices and edge attributes.\n",
    "        \"\"\"\n",
    "        if self.is_directed(edge_index):\n",
    "            edge_index_dup = torch.stack(\n",
    "                [edge_index[1, :], edge_index[0, :]],\n",
    "                dim=0\n",
    "            )   # (2, E)\n",
    "            edge_index = torch.cat(\n",
    "                [edge_index, edge_index_dup],\n",
    "                dim=1\n",
    "            )   # (2, 2*E)\n",
    "            edge_attr = torch.cat(\n",
    "                [edge_attr, edge_attr],\n",
    "                dim=0\n",
    "            )   # (2*E, fe)\n",
    "\n",
    "            return edge_index, edge_attr\n",
    "        else:\n",
    "            return edge_index, edge_attr\n",
    "\n",
    "    def data_gatherer(self, data):\n",
    "        \"\"\"\n",
    "        This method gathers the node features, prediction mask, edge index,\n",
    "        and edge features for both heterogeneous and homogeneous graph formats.\n",
    "        \"\"\"\n",
    "        if isinstance(data, HeteroData):\n",
    "            x = data[\"bus\"].x  # (N, 16)\n",
    "            mask = x[:, -6:]    # (N, 6): prediction mask (last 6 dims)\n",
    "            x = x[:, :-6]       # (N, 10): remaining features (4 one-hot + 6 real features)\n",
    "\n",
    "            edge_index = data[\"bus\", \"branch\", \"bus\"].edge_index\n",
    "            edge_features = data[\"bus\", \"branch\", \"bus\"].edge_attr\n",
    "\n",
    "        elif isinstance(data, Data):\n",
    "            # PowerFlowNet original format\n",
    "            assert data.x.shape[-1] == self.nfeature_dim * 2 + 4\n",
    "            x = data.x[:, 4:4 + self.nfeature_dim]\n",
    "            mask = data.x[:, -self.nfeature_dim:]\n",
    "            edge_index = data.edge_index\n",
    "            edge_features = data.edge_attr\n",
    "\n",
    "        return x, mask, edge_index, edge_features\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the PowerFlowNet.\n",
    "        Params:\n",
    "            data (Data): Input graph data containing:\n",
    "                - x (torch.Tensor): Node features (num_nodes, nfeature_dim).\n",
    "                - edge_index (torch.Tensor): Edge indices (2, num_edges).\n",
    "                - edge_attr (torch.Tensor): Edge features (num_edges, efeature_dim).\n",
    "                - pred_mask (torch.Tensor): Mask for features to predict.\n",
    "                - bus_type (torch.Tensor): Node types.\n",
    "                - batch (torch.Tensor): Batch information.\n",
    "        Returns:\n",
    "            (torch.Tensor): Output node embeddings (num_nodes, output_dim).\n",
    "        \"\"\"\n",
    "        x, mask, edge_index, edge_features = self.data_gatherer(data)\n",
    "\n",
    "        # assert data.x.shape[-1] == self.nfeature_dim * 2 + 4 # features and their mask + one-hot node type embedding\n",
    "        # x = data.x[:, 4:4+self.nfeature_dim] # first four features: node type. not elegant at all this way. just saying.\n",
    "\n",
    "        x = self.mask_embd(mask) + x\n",
    "\n",
    "        edge_index, edge_features = self.undirect_graph(edge_index, edge_features)\n",
    "\n",
    "        for i in range(len(self.layers)-1):\n",
    "            if isinstance(self.layers[i], EdgeAggregation):\n",
    "                x = self.layers[i](x=x, edge_index=edge_index, edge_attr=edge_features)\n",
    "            else:\n",
    "                x = self.layers[i](x=x, edge_index=edge_index)\n",
    "            x = self.dropout(x)\n",
    "            x = nn.ReLU()(x)\n",
    "\n",
    "        # Is this if statement necessary? It must be an EdgeAggregation layer\n",
    "        if isinstance(self.layers[-1], EdgeAggregation):\n",
    "            x = self.layers[-1](x=x, edge_index=edge_index, edge_attr=edge_features)\n",
    "        else:\n",
    "            x = self.layers[-1](x=x, edge_index=edge_index)\n",
    "\n",
    "        # # Mask out known values\n",
    "        # x = place_known_values(x, data)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49630ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_powerflownet(model, loader, optimizer, device='cpu', epochs=100):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for data in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(data)  # shape: [num_nodes, 6]\n",
    "\n",
    "            # Target values\n",
    "            y = data['bus'].y  # shape: [num_nodes, 6]\n",
    "            mask = data['bus'].x[:, -6:]  # prediction mask: [num_nodes, 6]\n",
    "\n",
    "            # Compute L2 loss only on masked targets\n",
    "            loss = F.mse_loss(out * mask, y * mask)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg MSE Loss = {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca72f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PowerFlowNet(\n",
    "    nfeature_dim=6,\n",
    "    efeature_dim=5,\n",
    "    output_dim=6,\n",
    "    hidden_dim=64,\n",
    "    n_gnn_layers=3,\n",
    "    K=3,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_powerflownet(model, train_loader, optimizer, device='cuda', epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
