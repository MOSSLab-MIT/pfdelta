{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f33c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06f817",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess for GNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7620622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.force_reload = force_reload\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()]) \n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        PQ_bus_x, PQ_bus_y = [], []\n",
    "        PV_bus_x, PV_bus_y = [], []\n",
    "        PV_demand, PV_generation = [], []\n",
    "        slack_x, slack_y = [], []\n",
    "        slack_demand, slack_generation = [], []\n",
    "        bus_x = []\n",
    "\n",
    "        PV_to_bus, PQ_to_bus, slack_to_bus = [], [], []\n",
    "        pq_idx, pv_idx, slack_idx = 0, 0, 0\n",
    "        gen_limits, gen_setpoints, more_gen_data  = [], [], []\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "\n",
    "            # Shunts \n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_x.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "                if int(gen['gen_bus']) == bus_id: \n",
    "                    if gen['gen_status'] == 1:\n",
    "                        gen_sol = solution_data['gen'][gen_id]\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            # Node features\n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            if bus['bus_type'] == 1:\n",
    "                PQ_bus_x.append(torch.tensor([pd, qd]))\n",
    "                PQ_bus_y.append(torch.tensor([va, vm]))\n",
    "                PQ_to_bus.append(torch.tensor([pq_idx, bus_idx]))\n",
    "                pq_idx += 1\n",
    "            elif bus['bus_type'] == 2:\n",
    "                PV_bus_x.append(torch.tensor([pg - pd, vm]))\n",
    "                PV_bus_y.append(torch.tensor([qg - qd, va]))\n",
    "                PV_demand.append(torch.tensor([pd, qd]))\n",
    "                PV_generation.append(torch.tensor([pg, qg]))\n",
    "                PV_to_bus.append(torch.tensor([pv_idx, bus_idx]))\n",
    "                pv_idx += 1\n",
    "            elif bus['bus_type'] == 3:\n",
    "                slack_x.append(torch.tensor([va, vm]))\n",
    "                slack_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                slack_demand.append(torch.tensor([pd, qd]))\n",
    "                slack_generation.append(torch.tensor([pg, qg]))\n",
    "                slack_to_bus.append(torch.tensor([slack_idx, bus_idx]))\n",
    "                slack_idx += 1\n",
    "        \n",
    "        # Generator\n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_sol = solution_data['gen'][gen_id]\n",
    "                pmin, pmax, qmin, qmax = gen['pmin'], gen['pmax'], gen['qmin'], gen['qmax']\n",
    "                pgen, qgen = gen_sol['pg'], gen_sol['qg']\n",
    "                gen_limits.append(torch.tensor([pmin, pmax, qmin, qmax]))\n",
    "                gen_setpoints.append(torch.tensor([pgen, qgen]))\n",
    "                is_slack = torch.tensor(\n",
    "                        1 if network_data['bus'][str(gen['gen_bus'])]['bus_type'] == 3 else 0,\n",
    "                        dtype=torch.bool\n",
    "                                )\n",
    "                gen_bus = torch.tensor(gen['gen_bus']) - 1  # zero-indexed\n",
    "                more_gen_data.append(torch.stack([gen_bus, is_slack]))\n",
    "            else:\n",
    "                assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "        # Edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            if branch['br_status'] == 0:\n",
    "                continue  # Skip inactive branches\n",
    "\n",
    "            from_bus = int(branch['f_bus']) - 1 \n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'], \n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            assert branch_sol is not None, f\"Missing solution for active branch {branch_id_str}\"\n",
    "\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['PQ'].x = torch.stack(PQ_bus_x) \n",
    "        data['PQ'].y = torch.stack(PQ_bus_y) \n",
    "\n",
    "        data['PV'].x = torch.stack(PV_bus_x) \n",
    "        data['PV'].y = torch.stack(PV_bus_y) \n",
    "        data['PV'].generation = torch.stack(PV_generation) \n",
    "        data['PV'].demand = torch.stack(PV_demand) \n",
    "\n",
    "        data['slack'].x = torch.stack(slack_x) \n",
    "        data['slack'].y = torch.stack(slack_y) \n",
    "        data['slack'].generation = torch.stack(slack_generation) \n",
    "        data['slack'].demand = torch.stack(slack_demand) \n",
    "\n",
    "        data['bus'].x = torch.stack(bus_x)\n",
    "\n",
    "        data['gen'].limits = torch.stack(gen_limits)\n",
    "        data['gen'].setpoints = torch.stack(gen_setpoints)\n",
    "        data['gen'].more_gen_data = torch.stack(more_gen_data)\n",
    "\n",
    "        data['bus', 'branch', 'bus'].edge_index = torch.stack(edge_index, dim=1) \n",
    "        data['bus', 'branch', 'bus'].edge_attr = torch.stack(edge_attr) \n",
    "        data['bus', 'branch', 'bus'].edge_label = torch.stack(edge_label) \n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('PV', 'PV_link', 'bus'): PV_to_bus,\n",
    "            ('PQ', 'PQ_link', 'bus'): PQ_to_bus,\n",
    "            ('slack', 'slack_link', 'bus'): slack_to_bus\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1) \n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):]\n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7373510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaGNS(PFDeltaDataset): \n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__(root_dir, case_name, split, transform, pre_transform, pre_filter, force_reload)\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        \"\"\" \"\"\"\n",
    "        # call base version\n",
    "        data = super().build_heterodata(pm_case)\n",
    "        num_buses = data['bus'].x.size(0)\n",
    "\n",
    "        # Init bus-level fields\n",
    "        v_buses      = torch.zeros(num_buses)\n",
    "        theta_buses  = torch.zeros(num_buses)\n",
    "        pd_buses     = torch.zeros(num_buses)\n",
    "        qd_buses     = torch.zeros(num_buses)\n",
    "        pg_buses     = torch.zeros(num_buses)\n",
    "        qg_buses     = torch.zeros(num_buses)\n",
    "\n",
    "        for ntype in ['PQ', 'PV', 'slack']:\n",
    "            node = data[ntype]\n",
    "            num_nodes = node.x.size(0)\n",
    "\n",
    "            if ntype == 'PQ':\n",
    "                # Flat start init\n",
    "                node.x_gns = torch.tensor([1.0, 0.0]).repeat(num_nodes, 1)\n",
    "                pd = node.x[:, 0]\n",
    "                qd = node.x[:, 1]\n",
    "                pg = torch.zeros_like(pd)\n",
    "                qg = torch.zeros_like(qd)\n",
    "\n",
    "            elif ntype == 'PV':\n",
    "                v = node.x[:, 1:2]\n",
    "                theta = torch.zeros_like(v)\n",
    "                node.x_gns = torch.cat([v, theta], dim=1)\n",
    "                pd = node.demand[:, 0]\n",
    "                qd = node.demand[:, 1]\n",
    "                pg = node.generation[:, 0]\n",
    "                qg = node.generation[:, 1]\n",
    "\n",
    "            elif ntype == 'slack':\n",
    "                node.x_gns = node.x.clone()\n",
    "                pd = node.demand[:, 0]\n",
    "                qd = node.demand[:, 1]\n",
    "                pg = node.generation[:, 0]\n",
    "                qg = node.generation[:, 1]\n",
    "\n",
    "            # Map to buses\n",
    "            link_type = ntype + '_link'\n",
    "            edge_index = data[(ntype, link_type, 'bus')].edge_index\n",
    "            src, dst = edge_index  # src is local node index, dst is bus index\n",
    "\n",
    "            x_gns = node.x_gns\n",
    "            v_dst = x_gns[:, 0]\n",
    "            theta_dst = x_gns[:, 1]\n",
    "\n",
    "            v_buses.index_add_(0, dst, v_dst)\n",
    "            theta_buses.index_add_(0, dst, theta_dst)\n",
    "            pd_buses.index_add_(0, dst, pd)\n",
    "            qd_buses.index_add_(0, dst, qd)\n",
    "            pg_buses.index_add_(0, dst, pg)\n",
    "            qg_buses.index_add_(0, dst, qg)\n",
    "\n",
    "        # Store in bus\n",
    "        data['bus'].v = v_buses\n",
    "        data['bus'].theta = theta_buses\n",
    "        data['bus'].pd = pd_buses\n",
    "        data['bus'].qd = qd_buses\n",
    "        data['bus'].pg = pg_buses\n",
    "        data['bus'].qg = qg_buses\n",
    "        data['bus'].delta_p = torch.zeros_like(v_buses)\n",
    "        data['bus'].delta_q = torch.zeros_like(v_buses)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13ef024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train (8091 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 8091/8091 [00:49<00:00, 163.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: val (1011 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 1011/1011 [00:06<00:00, 147.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test (1012 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 1012/1012 [00:06<00:00, 156.46it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "case_14_data = PFDeltaGNS(root_dir='data/gns_data', case_name='case14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d0752b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.1900],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000]]), 'v': tensor([0.0000, 1.0461, 0.9502, 1.0000, 1.0000, 1.0600, 1.0000, 1.0600, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000]), 'theta': tensor([1.0600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'pd': tensor([0.0000, 0.2017, 0.9490, 0.4597, 0.0744, 0.1209, 0.0000, 0.0000, 0.2653,\n",
       "        0.1063, 0.0337, 0.0683, 0.1415, 0.1516]), 'qd': tensor([0.0000, 0.0650, 0.4116, 0.1102, 0.0380, 0.0814, 0.0000, 0.0000, 0.1732,\n",
       "        0.0652, 0.0221, 0.0325, 0.1024, 0.0360]), 'pg': tensor([2.1193, 0.5900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'qg': tensor([-0.0605,  0.8895,  0.0000,  0.0000,  0.0000,  0.2793,  0.0000,  0.1480,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]), 'delta_p': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'delta_q': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_14_data[10]['bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7efd1b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[0, 1, 2, 3],\n",
       "        [1, 2, 5, 7]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_14_data[0][\"PV\", \"PV_link\", \"bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45d0b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_14_train = torch.load('data/gns_data/case14/processed/train.pt')\n",
    "case_14_val = torch.load('data/gns_data/case14/processed/val.pt')\n",
    "case_14_test = torch.load('data/gns_data/case14/processed/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7057da9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  PQ={\n",
       "    x=[72819, 2],\n",
       "    y=[72819, 2],\n",
       "    x_gns=[72819, 2],\n",
       "  },\n",
       "  PV={\n",
       "    x=[32364, 2],\n",
       "    y=[32364, 2],\n",
       "    generation=[32364, 2],\n",
       "    demand=[32364, 2],\n",
       "    x_gns=[32364, 2],\n",
       "  },\n",
       "  slack={\n",
       "    x=[8091, 2],\n",
       "    y=[8091, 2],\n",
       "    generation=[8091, 2],\n",
       "    demand=[8091, 2],\n",
       "    x_gns=[8091, 2],\n",
       "  },\n",
       "  bus={\n",
       "    x=[113274, 2],\n",
       "    v=[113274],\n",
       "    theta=[113274],\n",
       "  },\n",
       "  gen={\n",
       "    limits=[517748, 4],\n",
       "    setpoints=[517748, 2],\n",
       "  },\n",
       "  (bus, branch, bus)={\n",
       "    edge_index=[2, 157202],\n",
       "    edge_attr=[157202, 8],\n",
       "    edge_label=[157202, 4],\n",
       "  },\n",
       "  (PV, PV_link, bus)={ edge_index=[2, 32364] },\n",
       "  (bus, PV_link, PV)={ edge_index=[2, 32364] },\n",
       "  (PQ, PQ_link, bus)={ edge_index=[2, 72819] },\n",
       "  (bus, PQ_link, PQ)={ edge_index=[2, 72819] },\n",
       "  (slack, slack_link, bus)={ edge_index=[2, 8091] },\n",
       "  (bus, slack_link, slack)={ edge_index=[2, 8091] }\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_14_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b7f38",
   "metadata": {},
   "source": [
    "### Graph Neural Solver Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralSolver(nn.Module):\n",
    "    def __init__(self, K, hidden_dim, L_input_dim, phi_input_dim, gamma, data):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.gamma = gamma\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(phi_input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            NNUpdate(L_input_dim, hidden_dim) for _ in range(K)\n",
    "        )\n",
    "        \n",
    "        # Instantiate message vectors for each bus\n",
    "        num_nodes = data['bus'].x.size(0)\n",
    "        data['bus'].m = torch.zeros((num_nodes, hidden_dim))\n",
    "\n",
    "\n",
    "    def forward(self, data):  \n",
    "        \"\"\" \"\"\"\n",
    "        total_layer_loss = 0\n",
    "\n",
    "        for k in range(self.K): \n",
    "\n",
    "            # Update P and Q values for all buses\n",
    "            self.global_active_compensation(data) \n",
    "\n",
    "            # Compute local power imbalance variables and store power imbalance loss \n",
    "            layer_loss = self.local_power_imbalance(data)\n",
    "            total_layer_loss += layer_loss * torch.pow(self.gamma, self.K - k)\n",
    "\n",
    "            # Apply the neural network update block \n",
    "            self.apply_nn_update(data, k)\n",
    "        \n",
    "        return data, total_layer_loss\n",
    "\n",
    "\n",
    "    def global_active_compensation(self, data):\n",
    "        \"\"\" \"\"\"\n",
    "        # Compute global power demand \n",
    "        p_joule = self.compute_p_joule(data)   \n",
    "        p_global = self.compute_p_global(data, p_joule)\n",
    "\n",
    "        # Compute pg_slack and assign new pg value\n",
    "        pg_slack = self.compute_pg_slack(p_global, data)\n",
    "        data['bus'].pg[0] = pg_slack\n",
    "        # TODO: check if the remaining buses need to be kept the same \n",
    "\n",
    "        # Compute qg values for each bus\n",
    "        self.compute_qg(data)\n",
    "        \n",
    "\n",
    "    def compute_p_joule(self, data): \n",
    "        \"\"\" \"\"\"\n",
    "        # Extract edge index and attributes\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr \n",
    "        src, dst = edge_index \n",
    "\n",
    "        # Edge features\n",
    "        tau_ij = edge_attr[:, 6]\n",
    "        shift_ij = edge_attr[: 7]\n",
    "\n",
    "        # Line admittance features \n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        y = 1 / (torch.complex(br_r, br_x))\n",
    "        y_ij = torch.abs(y)\n",
    "        delta_ij = torch.angle(y)\n",
    "\n",
    "        # Node features\n",
    "        v_i = data['bus'].v[src]\n",
    "        v_j = data['bus'].v[dst]\n",
    "        theta_i = data['bus'].theta[src]\n",
    "        theta_j = data['bus'].theta[dst]\n",
    "\n",
    "        # Compute p_global\n",
    "        term1 = v_i * v_j * y_ij / tau_ij * (\n",
    "            torch.sin(theta_i - theta_j - delta_ij - shift_ij) +\n",
    "            torch.sin(theta_j - theta_i - delta_ij + shift_ij)\n",
    "        )\n",
    "\n",
    "        term2 = (v_i / tau_ij) ** 2 * y_ij * torch.sin(delta_ij)\n",
    "        term3 = v_j ** 2 * y_ij * torch.sin(delta_ij)\n",
    "\n",
    "        p_joule = torch.abs(term1 + term2 + term3).sum()\n",
    "\n",
    "        return p_joule\n",
    "    \n",
    "    def compute_p_global(self, data, p_joule): \n",
    "        \"\"\" \"\"\"\n",
    "        # Extract relevant variables for computation \n",
    "        pd = data['bus'].pd \n",
    "        v = data['bus'].v\n",
    "        g_shunt = data['bus'].x[:, 0]\n",
    "\n",
    "        # Compute p_global\n",
    "        p_global = (pd + (v ** 2) * g_shunt).sum() + p_joule\n",
    "        \n",
    "        return p_global\n",
    "\n",
    "    def compute_pg_slack(self, p_global, data):\n",
    "        \"\"\" \"\"\"\n",
    "        pg_setpoints = data['gen'].setpoints[:, 0] \n",
    "        pg_max_vals = data['gen'].limits[:, 1]\n",
    "        pg_min_vals = data['gen'].limits[:, 0]\n",
    "        is_slack = data['gen'].more_gen_data[:, 1] == 1 \n",
    "        pg_setpoints_non_slack = pg_setpoints[~is_slack]\n",
    "        pg_setpoint_slack = pg_setpoints[is_slack]\n",
    "        pg_max_slack = pg_max_vals[is_slack] \n",
    "        pg_min_slack = pg_min_vals[is_slack]  \n",
    "        \n",
    "        if p_global < pg_setpoints.sum(): \n",
    "            lamb = (p_global - pg_setpoints_non_slack.sum() - pg_max_slack)\n",
    "            lamb = lamb / 2*(pg_setpoint_slack - pg_min_slack)\n",
    "        else: \n",
    "            lamb = (p_global - pg_setpoints_non_slack.sum() - 2*pg_setpoint_slack - pg_max_slack)\n",
    "            lamb = lamb / 2*(pg_max_slack - pg_setpoint_slack)\n",
    "        \n",
    "        lamb = torch.max(0, lamb)\n",
    "        if lamb < 1/2: \n",
    "            pg_slack = pg_min_slack + 2 * (pg_setpoint_slack - pg_min_slack) * lamb\n",
    "        else: \n",
    "            pg_slack = 2 * pg_setpoint_slack - pg_max_slack + 2 * (pg_max_slack - pg_setpoint_slack) * lamb\n",
    "        \n",
    "        return pg_slack\n",
    "    \n",
    "    def compute_qg(self, data): \n",
    "        pass \n",
    "\n",
    "    def local_power_imbalance(self, data):\n",
    "        # compute delta P\n",
    "        # compute delta Q\n",
    "        # compute delta S -- store these values in the data instance and scale by gamma factor\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def apply_nn_update(self, data, k): \n",
    "        # apply phi to the message vector at each node and on the line values \n",
    "        # apply the k-th layer NN udpate \n",
    "        self.layers[k] # pass inputs in\n",
    "        pass\n",
    "\n",
    "\n",
    "class NNUpdate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L_theta = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_v = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_m = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # apply V, theta, and m updates within the data itself\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def kirchoff_law_violation_loss(layer_loss, data):\n",
    "    # get loss per batch, avergae across samples and then backprop\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
