{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f33c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06f817",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess for GNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.force_reload = force_reload\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()])\n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        PQ_bus_x, PQ_bus_y = [], []\n",
    "        PV_bus_x, PV_bus_y = [], []\n",
    "        PV_demand, PV_generation = [], []\n",
    "        slack_x, slack_y = [], []\n",
    "        slack_demand, slack_generation = [], []\n",
    "        bus_x = []\n",
    "\n",
    "        PV_to_bus, PQ_to_bus, slack_to_bus = [], [], []\n",
    "        pq_idx, pv_idx, slack_idx = 0, 0, 0\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "\n",
    "            # Shunts\n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_x.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in network_data['gen'].items():\n",
    "                if int(gen['gen_bus']) == bus_id:\n",
    "                    gen_sol = solution_data['gen'].get(gen_id)\n",
    "                    if gen_sol:\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert gen['gen_status'] == 0, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            # Node features\n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            if bus['bus_type'] == 1:\n",
    "                PQ_bus_x.append(torch.tensor([pd, qd]))\n",
    "                PQ_bus_y.append(torch.tensor([va, vm]))\n",
    "                PQ_to_bus.append(torch.tensor([pq_idx, bus_idx]))\n",
    "                pq_idx += 1\n",
    "            elif bus['bus_type'] == 2:\n",
    "                PV_bus_x.append(torch.tensor([pg - pd, vm]))\n",
    "                PV_bus_y.append(torch.tensor([qg - qd, va]))\n",
    "                PV_demand.append(torch.tensor([pd, qd]))\n",
    "                PV_generation.append(torch.tensor([pg, qg]))\n",
    "                PV_to_bus.append(torch.tensor([pv_idx, bus_idx]))\n",
    "                pv_idx += 1\n",
    "            elif bus['bus_type'] == 3:\n",
    "                slack_x.append(torch.tensor([va, vm]))\n",
    "                slack_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                slack_demand.append(torch.tensor([pd, qd]))\n",
    "                slack_generation.append(torch.tensor([pg, qg]))\n",
    "                slack_to_bus.append(torch.tensor([slack_idx, bus_idx]))\n",
    "                slack_idx += 1\n",
    "\n",
    "        # Edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            from_bus = int(branch['f_bus']) - 1\n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'],\n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "            else:\n",
    "                assert branch['br_status'] == 0, f\"Expected branch {branch_id_str} to be outaged.\"\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['PQ'].x = torch.stack(PQ_bus_x)\n",
    "        data['PQ'].y = torch.stack(PQ_bus_y)\n",
    "\n",
    "        data['PV'].x = torch.stack(PV_bus_x)\n",
    "        data['PV'].y = torch.stack(PV_bus_y)\n",
    "        data['PV'].generation = torch.stack(PV_generation)\n",
    "        data['PV'].demand = torch.stack(PV_demand)\n",
    "\n",
    "        data['slack'].x = torch.stack(slack_x)\n",
    "        data['slack'].y = torch.stack(slack_y)\n",
    "        data['slack'].generation = torch.stack(slack_generation)\n",
    "        data['slack'].demand = torch.stack(slack_demand)\n",
    "\n",
    "        data['bus'].x = torch.stack(bus_x)\n",
    "\n",
    "        data['bus', 'branch', 'bus'].edge_index = torch.stack(edge_index, dim=1)\n",
    "        data['bus', 'branch', 'bus'].edge_attr = torch.stack(edge_attr)\n",
    "        data['bus', 'branch', 'bus'].edge_label = torch.stack(edge_label)\n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('PV', 'PV_link', 'bus'): PV_to_bus,\n",
    "            ('PQ', 'PQ_link', 'bus'): PQ_to_bus,\n",
    "            ('slack', 'slack_link', 'bus'): slack_to_bus\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1)\n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):]\n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaGNSDataset(PFDeltaDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__()\n",
    "        # self.split = split\n",
    "        # self.force_reload = force_reload\n",
    "        # root = os.path.join(root_dir, case_name)\n",
    "        # super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        # self.load(self.processed_paths[self._split_to_idx()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9af085e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['network', 'solution'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_path = 'gns_data/case14/raw/instance_1.json'\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print top-level keys or preview content\n",
    "print(type(data))\n",
    "print(data.keys() if isinstance(data, dict) else data[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6c4d63b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'slack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetwork\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mslack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'slack'"
     ]
    }
   ],
   "source": [
    "data['network']['slack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b13ef024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m case_14_data \u001b[38;5;241m=\u001b[39m \u001b[43mPFDeltaGNSDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgns_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcase14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m, in \u001b[0;36mPFDeltaGNSDataset.__init__\u001b[0;34m(self, root_dir, case_name, split, transform, pre_transform, pre_filter, force_reload)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgns_data\u001b[39m\u001b[38;5;124m'\u001b[39m, case_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mPFDeltaDataset.__init__\u001b[0;34m(self, root_dir, case_name, split, transform, pre_transform, pre_filter, force_reload)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[1;32m      5\u001b[0m root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, case_name)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_to_idx()])\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch_geometric/data/dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch_geometric/data/dataset.py:262\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    261\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mPFDeltaDataset.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     fnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m\n\u001b[1;32m    144\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(fnames)\n\u001b[1;32m    145\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(fnames)\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mPFDeltaDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraw_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw'"
     ]
    }
   ],
   "source": [
    "case_14_data = PFDeltaGNSDataset(root_dir='gns_data', case_name='case14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecba5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_14_train = torch.load('gns_data/case14/processed/train.pt')\n",
    "case_14_val = torch.load('gns_data/case14/processed/val.pt')\n",
    "case_14_test = torch.load('gns_data/case14/processed/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8970eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = case_14_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fb312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  PQ={\n",
       "    x=[9, 2],\n",
       "    y=[9, 2],\n",
       "  },\n",
       "  PV={\n",
       "    x=[4, 2],\n",
       "    y=[4, 2],\n",
       "    generation=[4, 2],\n",
       "    demand=[4, 2],\n",
       "  },\n",
       "  slack={\n",
       "    x=[1, 2],\n",
       "    y=[1, 2],\n",
       "    generation=[1, 2],\n",
       "    demand=[1, 2],\n",
       "  },\n",
       "  bus={ x=[14, 2] },\n",
       "  (bus, branch, bus)={\n",
       "    edge_index=[2, 20],\n",
       "    edge_attr=[20, 8],\n",
       "    edge_label=[19, 4],\n",
       "  },\n",
       "  (PV, PV_link, bus)={ edge_index=[2, 4] },\n",
       "  (bus, PV_link, PV)={ edge_index=[2, 4] },\n",
       "  (PQ, PQ_link, bus)={ edge_index=[2, 9] },\n",
       "  (bus, PQ_link, PQ)={ edge_index=[2, 9] },\n",
       "  (slack, slack_link, bus)={ edge_index=[2, 1] },\n",
       "  (bus, slack_link, slack)={ edge_index=[2, 1] }\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data\n",
    "# in the bus variables we have shunt susceptance and conductance \n",
    "# we need to get Pmin, Psetpoints, Pmax for all the PV buses and store them somewhere "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b7f38",
   "metadata": {},
   "source": [
    "### Graph Neural Solver Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralSolver(nn.Module):\n",
    "    def __init__(self, K, hidden_dim, L_input_dim, phi_input_dim, gamma):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.gamma = gamma\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(phi_input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            NNUpdate(L_input_dim, hidden_dim) for _ in range(K)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data):  \n",
    "        \"\"\" \"\"\"\n",
    "        total_layer_loss = 0\n",
    "\n",
    "        for k in range(self.K): \n",
    "\n",
    "            # Update P and Q values for all buses\n",
    "            self.global_active_compensation(data) \n",
    "\n",
    "            # Compute local power imbalance variables and store power imbalance loss \n",
    "            layer_loss = self.local_power_imbalance(data)\n",
    "            total_layer_loss += layer_loss * torch.pow(self.gamma, self.K - k)\n",
    "\n",
    "            # Apply the neural network update block \n",
    "            self.apply_nn_update(data, k)\n",
    "        \n",
    "        return data, total_layer_loss\n",
    "\n",
    "\n",
    "    def global_active_compensation(self, data):\n",
    "        # calculate global P_joule\n",
    "        # calculate P_global\n",
    "        p_joule = None \n",
    "        p_global = None\n",
    "\n",
    "        # Compute lambda\n",
    "        lamb = self.compute_lambda(p_global, data)\n",
    "        lamb = torch.max(0, lamb)\n",
    "\n",
    "        # calculate P_gi slack (using lambda factor)\n",
    "        # calculate P_gi non-slack buses\n",
    "\n",
    "        # calculate Q_gi for all buses\n",
    "        pass\n",
    "\n",
    "\n",
    "    def compute_lambda(self, p_global, data):\n",
    "        # simple scalar calculation\n",
    "        pass\n",
    "\n",
    "\n",
    "    def local_power_imbalance(self, data):\n",
    "        # compute delta P\n",
    "        # compute delta Q\n",
    "        # compute delta S -- store these values in the data instance and scale by gamma factor\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def apply_nn_update(self, data, k): \n",
    "        # apply phi to the message vector at each node and on the line values \n",
    "        # apply the k-th layer NN udpate \n",
    "        self.layers[k] # pass inputs in\n",
    "        pass\n",
    "\n",
    "\n",
    "class NNUpdate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L_theta = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_v = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_m = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # apply V, theta, and m updates within the data itself\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def kirchoff_law_violation_loss(layer_loss, data):\n",
    "    # get loss per batch, avergae across samples and then backprop\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
