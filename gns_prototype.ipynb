{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f33c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06f817",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess for GNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685f254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData\n",
    "\n",
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.case_name = case_name\n",
    "        self.force_reload = force_reload\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()]) \n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2, 'all': 3}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt', 'all.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        # Bus nodes\n",
    "        pf_x, pf_y = [], []\n",
    "        bus_gen, bus_demand = [], []\n",
    "        bus_voltages = []\n",
    "        bus_type = []\n",
    "        bus_shunts = []\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "            \n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            bus_voltages.append(torch.tensor([va, vm]))\n",
    "\n",
    "            # Shunts \n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_shunts.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "                if int(gen['gen_bus']) == bus_id: \n",
    "                    if gen['gen_status'] == 1:\n",
    "                        gen_sol = solution_data['gen'][gen_id]\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            # Now decide final bus type\n",
    "            bus_type_now = bus['bus_type']\n",
    "\n",
    "            if bus_type_now == 2 and pg == 0.0 and qg == 0.0:\n",
    "                bus_type_now = 1  # PV bus with no gen --> becomes PQ\n",
    "\n",
    "            bus_type.append(torch.tensor(bus_type_now))\n",
    "\n",
    "            if bus_type_now == 1:\n",
    "                pf_x.append(torch.tensor([pd, qd]))\n",
    "                pf_y.append(torch.tensor([va, vm]))\n",
    "                bus_gen.append(torch.tensor([pg, qg]))\n",
    "                bus_demand.append(torch.tensor([pd, qd]))\n",
    "            elif bus_type_now == 2:\n",
    "                pf_x.append(torch.tensor([pg - pd, vm]))\n",
    "                pf_y.append(torch.tensor([qg - qd, va]))\n",
    "                bus_gen.append(torch.tensor([pg, qg]))\n",
    "                bus_demand.append(torch.tensor([pd, qd]))\n",
    "            elif bus_type_now == 3:\n",
    "                pf_x.append(torch.tensor([va, vm]))\n",
    "                pf_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                bus_gen.append(torch.tensor([pg, qg]))\n",
    "                bus_demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "        generation, limits, slack_gen, slack_limits  = [], [], [], []\n",
    "\n",
    "        # Generator nodes\n",
    "        # Slack generator limits, [PMAX, PMIN]\n",
    "        if self.case_name == 'case14':\n",
    "            slack_limits.append(torch.tensor([3.4, 0.0]))\n",
    "        elif self.case_name == 'case57':\n",
    "            slack_limits.append(torch.tensor([2.45, 0.0]))\n",
    "        elif self.case_name == 'case118':\n",
    "            slack_limits.append(torch.tensor([11.82, 0.0]))\n",
    "            \n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_sol = solution_data['gen'][gen_id] \n",
    "                pmin, pmax, qmin, qmax = gen['pmin'], gen['pmax'], gen['qmin'], gen['qmax']\n",
    "                pgen, qgen = gen_sol['pg'], gen_sol['qg']\n",
    "                limits.append(torch.tensor([pmin, pmax, qmin, qmax]))\n",
    "                generation.append(torch.tensor([pgen, qgen]))\n",
    "                is_slack = torch.tensor(\n",
    "                        1 if network_data['bus'][str(gen['gen_bus'])]['bus_type'] == 3 else 0,\n",
    "                        dtype=torch.bool\n",
    "                                )\n",
    "                slack_gen.append(is_slack)\n",
    "            else:\n",
    "                assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "        # Load nodes\n",
    "        demand = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            pd, qd = load['pd'], load['qd']\n",
    "            demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "        # Edges\n",
    "\n",
    "        # bus to bus edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            if branch['br_status'] == 0:\n",
    "                continue  # Skip inactive branches\n",
    "\n",
    "            from_bus = int(branch['f_bus']) - 1 \n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'], \n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            assert branch_sol is not None, f\"Missing solution for active branch {branch_id_str}\"\n",
    "\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "\n",
    "        # bus to gen edges\n",
    "        gen_to_bus_index = []\n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_bus = torch.tensor(gen['gen_bus']) - 1\n",
    "                gen_to_bus_index.append(torch.tensor([int(gen_id) - 1, gen_bus]))\n",
    "\n",
    "        # bus to load edges\n",
    "        load_to_bus_index = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            load_bus = torch.tensor(load['load_bus']) - 1\n",
    "            load_to_bus_index.append(torch.tensor([int(load_id) - 1, load_bus]))\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['bus'].pf_x = torch.stack(pf_x)\n",
    "        data['bus'].pf_y = torch.stack(pf_y)\n",
    "        data['bus'].bus_gen = torch.stack(bus_gen)\n",
    "        data['bus'].bus_demand = torch.stack(bus_demand)\n",
    "        data['bus'].bus_voltages = torch.stack(bus_voltages)\n",
    "        data['bus'].bus_type = torch.stack(bus_type)\n",
    "        data['bus'].shunt = torch.stack(bus_shunts)\n",
    "\n",
    "        data['gen'].limits = torch.stack(limits)\n",
    "        data['gen'].generation = torch.stack(generation)\n",
    "        data['gen'].slack_gen = torch.stack(slack_gen)\n",
    "        data['gen'].slack_limits = torch.stack(slack_limits)\n",
    "\n",
    "        data['load'].demand = torch.stack(demand)\n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('bus', 'branch', 'bus'): edge_index,\n",
    "            ('gen', 'gen_link', 'bus'): gen_to_bus_index,\n",
    "            ('load', 'load_link', 'bus'): load_to_bus_index\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1) \n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "            if link_name == ('bus', 'branch', 'bus'): \n",
    "                data[link_name].edge_attr = torch.stack(edge_attr) \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):],\n",
    "            'all': fnames  \n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23edcb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaGNS(PFDeltaDataset): \n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__(root_dir, case_name, split, transform, pre_transform, pre_filter, force_reload)\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        # call base version\n",
    "        data = super().build_heterodata(pm_case)\n",
    "        num_buses = data['bus'].pf_x.size(0)\n",
    "\n",
    "        # Init bus-level fields\n",
    "        v_buses      = torch.zeros(num_buses)\n",
    "        theta_buses  = torch.zeros(num_buses)\n",
    "        pd_buses     = torch.zeros(num_buses)\n",
    "        qd_buses     = torch.zeros(num_buses)\n",
    "        pg_buses     = torch.zeros(num_buses)\n",
    "        qg_buses     = torch.zeros(num_buses)\n",
    "\n",
    "        # Read bus types\n",
    "        bus_types = data['bus'].bus_type \n",
    "        x_gns = torch.zeros((num_buses, 2))\n",
    "\n",
    "        for bus_idx in range(num_buses):\n",
    "            bus_type = bus_types[bus_idx].item()\n",
    "            pf_x = data['bus'].pf_x[bus_idx]\n",
    "            pf_y = data['bus'].pf_y[bus_idx]\n",
    "            bus_demand = data['bus'].bus_demand[bus_idx]\n",
    "            bus_gen = data['bus'].bus_gen[bus_idx]\n",
    "\n",
    "            if bus_type == 1:  # PQ bus\n",
    "                # Flat start for PQ bus\n",
    "                x_gns[bus_idx] = torch.tensor([1.0, 0.0])\n",
    "                pd = pf_x[0]\n",
    "                qd = pf_x[1]\n",
    "                pg = bus_gen[0]\n",
    "                qg = bus_gen[1]\n",
    "            elif bus_type == 2:  # PV bus\n",
    "                v = pf_x[1]\n",
    "                theta = torch.tensor(0.0)\n",
    "                x_gns[bus_idx] = torch.stack([v, theta])\n",
    "                pd = bus_demand[0]\n",
    "                qd = bus_demand[1]\n",
    "                pg = bus_gen[0]\n",
    "                qg = bus_gen[1]\n",
    "            elif bus_type == 3:  # Slack bus\n",
    "                x_gns[bus_idx] = pf_x\n",
    "                pd = bus_demand[0]\n",
    "                qd = bus_demand[1]\n",
    "                pg = bus_gen[0]\n",
    "                qg = bus_gen[1]\n",
    "\n",
    "            v_buses[bus_idx] = x_gns[bus_idx][0]\n",
    "            theta_buses[bus_idx] = x_gns[bus_idx][1]\n",
    "            pd_buses[bus_idx] = pd\n",
    "            qd_buses[bus_idx] = qd\n",
    "            pg_buses[bus_idx] = pg\n",
    "            qg_buses[bus_idx] = qg\n",
    "\n",
    "        # Store in bus\n",
    "        data['bus'].x_gns = x_gns\n",
    "        data['bus'].v = v_buses\n",
    "        data['bus'].theta = theta_buses\n",
    "        data['bus'].pd = pd_buses\n",
    "        data['bus'].qd = qd_buses\n",
    "        data['bus'].pg = pg_buses\n",
    "        data['bus'].qg = qg_buses\n",
    "        data['bus'].delta_p = torch.zeros_like(v_buses)\n",
    "        data['bus'].delta_q = torch.zeros_like(v_buses)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c36cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_14_data = PFDeltaGNS(root_dir='data/gns_data/case14_n/', case_name='case14', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b7f38",
   "metadata": {},
   "source": [
    "### Graph Neural Solver Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42dee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralSolver(nn.Module):\n",
    "    def __init__(self, K, hidden_dim, gamma):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.gamma = gamma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.phi_input_dim = hidden_dim + 5\n",
    "        self.L_input_dim = 2 * hidden_dim + 4\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(self.phi_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            NNUpdate(self.L_input_dim, hidden_dim) for _ in range(K)\n",
    "        )\n",
    "        self.power_balance = LocalPowerBalanceLoss()\n",
    "\n",
    "    def forward(self, data):  \n",
    "        \"\"\" \"\"\"\n",
    "        device = data['bus'].pf_x.device \n",
    "\n",
    "        # Instantiate message vectors for each bus\n",
    "        num_nodes = data['bus'].pf_x.size(0)\n",
    "        data['bus'].m = torch.zeros((num_nodes, self.hidden_dim), device=device)\n",
    "        total_layer_loss = 0\n",
    "\n",
    "        for k in range(self.K): \n",
    "\n",
    "            # Update P and Q values for all buses\n",
    "            self.global_active_compensation(data) \n",
    "\n",
    "            # Compute local power imbalance variables and store power imbalance loss \n",
    "            layer_loss = self.local_power_imbalance(data, layer_loss=True)\n",
    "            total_layer_loss += layer_loss * (self.gamma ** (self.K - k))\n",
    "\n",
    "            # Apply the neural network update block \n",
    "            self.apply_nn_update(data, k)\n",
    "        \n",
    "        return data, total_layer_loss\n",
    "\n",
    "\n",
    "    def global_active_compensation(self, data):\n",
    "        \"\"\" \"\"\"\n",
    "        # Compute global power demand \n",
    "        p_joule = self.compute_p_joule(data)   \n",
    "        p_global = self.compute_p_global(data, p_joule)\n",
    "\n",
    "        # Compute pg_slack and assign to relevant buses\n",
    "        pg_slack = self.compute_pg_slack(p_global, data)\n",
    "        slack_idx = (data['bus'].bus_type == 3).nonzero(as_tuple=True)[0]\n",
    "        data['bus'].pg[slack_idx] = pg_slack\n",
    "\n",
    "        # Compute qg values for each bus\n",
    "        qg = self.power_balance(data, layer_loss=False)\n",
    "        data['bus'].qg = qg\n",
    "\n",
    "\n",
    "    def compute_p_joule(self, data): \n",
    "        \"\"\" \"\"\"\n",
    "        # Extract edge index and attributes\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr \n",
    "        src, dst = edge_index \n",
    "\n",
    "        # Edge features\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "\n",
    "        # Line admittance features \n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        y = 1 / (torch.complex(br_r, br_x))\n",
    "        y_ij = torch.abs(y)\n",
    "        delta_ij = torch.angle(y)\n",
    "\n",
    "        # Node features\n",
    "        v_i = data['bus'].v[src]\n",
    "        v_j = data['bus'].v[dst]\n",
    "        theta_i = data['bus'].theta[src]\n",
    "        theta_j = data['bus'].theta[dst]\n",
    "\n",
    "        # Compute p_global\n",
    "        term1 = v_i * v_j * y_ij / tau_ij * (\n",
    "            torch.sin(theta_i - theta_j - delta_ij - shift_ij) +  \n",
    "            torch.sin(theta_j - theta_i - delta_ij + shift_ij))\n",
    "\n",
    "        term2 = (v_i / tau_ij) ** 2 * y_ij * torch.sin(delta_ij)\n",
    "        term3 = v_j ** 2 * y_ij * torch.sin(delta_ij)\n",
    "        p_joule_edge = torch.abs(term1 + term2 + term3)\n",
    "        \n",
    "        # Map to individual graphs\n",
    "        bus_batch = data['bus'].batch  # batch index per bus\n",
    "        edge_batch = bus_batch[src]    # batch index per edge (via source bus)\n",
    "        num_graphs = int(edge_batch.max()) + 1\n",
    "        p_joule_per_graph = torch.zeros(num_graphs, device=p_joule_edge.device)\n",
    "        p_joule_per_graph = p_joule_per_graph.index_add(0, edge_batch, p_joule_edge)\n",
    "\n",
    "        return p_joule_per_graph\n",
    "    \n",
    "\n",
    "    def compute_p_global(self, data, p_joule): \n",
    "        \"\"\" \"\"\"\n",
    "        # Per-bus data\n",
    "        pd = data['bus'].pd \n",
    "        v = data['bus'].v\n",
    "        g_shunt = data['bus'].shunt[:, 0]\n",
    "\n",
    "        # Graph assignment per bus\n",
    "        bus_batch = data['bus'].batch\n",
    "        num_graphs = int(bus_batch.max()) + 1\n",
    "\n",
    "        # Compute local p_global components\n",
    "        p_global_local = pd + (v ** 2) * g_shunt\n",
    "\n",
    "        # Sum per graph\n",
    "        p_global = torch.zeros(num_graphs, device=p_global_local.device)\n",
    "        p_global = p_global.index_add(0, bus_batch, p_global_local)\n",
    "\n",
    "        # Add per-graph Joule losses\n",
    "        p_global += p_joule\n",
    "\n",
    "        return p_global\n",
    "\n",
    "\n",
    "    def compute_pg_slack(self, p_global, data):\n",
    "        \"\"\" \"\"\"\n",
    "        pg_setpoints = data['gen'].generation[:, 0] \n",
    "        pg_max_vals = data['gen'].limits[:, 1]\n",
    "        pg_min_vals = data['gen'].limits[:, 0]\n",
    "        pmax_slack = data['gen'].slack_limits[:, 0]\n",
    "        pmin_slack = data['gen'].slack_limits[:, 1]\n",
    "\n",
    "        is_slack = data['gen'].slack_gen\n",
    "        graph_ids = is_slack.cumsum(dim=0) - 1\n",
    "        num_graphs = graph_ids.max().item() + 1\n",
    "        \n",
    "        pg_setpoint_slack = pg_setpoints[is_slack]\n",
    "        pg_setpoints_non_slack = pg_setpoints.clone()\n",
    "        pg_setpoints_non_slack[is_slack] = 0.0\n",
    "        pg_max_slack = torch.full_like(pg_max_vals[is_slack], pmax_slack)\n",
    "        pg_min_slack = torch.full_like(pg_min_vals[is_slack], pmin_slack)\n",
    "\n",
    "        # Sum of total generator setpoints per graph\n",
    "        pg_setpoints_sum = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        pg_setpoints_sum = pg_setpoints_sum.index_add(0, graph_ids, pg_setpoints)\n",
    "        pg_non_slack_setpoints_sum = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        pg_non_slack_setpoints_sum = pg_non_slack_setpoints_sum.index_add(0, graph_ids, pg_setpoints_non_slack)\n",
    "\n",
    "        # Compute lambda in a vectorized way\n",
    "        under = (p_global < pg_setpoints_sum)\n",
    "        over = ~under\n",
    "        lamb = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        lamb[under] = (\n",
    "            (p_global[under] - pg_non_slack_setpoints_sum[under] - pg_max_slack[under]) /\n",
    "            (2 * (pg_setpoint_slack[under] - pg_min_slack[under]))\n",
    "        )\n",
    "        lamb[over] = (\n",
    "            (p_global[over] - pg_non_slack_setpoints_sum[over] - 2 * pg_setpoint_slack[over] - pg_max_slack[over]) /\n",
    "            (2 * (pg_max_slack[over] - pg_setpoint_slack[over]))\n",
    "        )\n",
    "\n",
    "        lamb = torch.clamp(lamb, min=0.0)\n",
    "        pg_slack = torch.zeros_like(lamb)\n",
    "\n",
    "        # Compute the pg_slack values \n",
    "        case1 = lamb < 0.5\n",
    "        case2 = ~case1\n",
    "        pg_slack[case1] = pg_min_slack[case1] + 2 * (pg_setpoint_slack[case1] - pg_min_slack[case1]) * lamb[case1]\n",
    "\n",
    "        pg_slack[case2] = (\n",
    "            2 * pg_setpoint_slack[case2] - pg_max_slack[case2] +\n",
    "            2 * (pg_max_slack[case2] - pg_setpoint_slack[case2]) * lamb[case2]\n",
    "        )\n",
    "        \n",
    "        return pg_slack\n",
    "\n",
    "\n",
    "    def local_power_imbalance(self, data, layer_loss):\n",
    "        \"\"\" \"\"\"\n",
    "        delta_p, delta_q, delta_s = self.power_balance(data, layer_loss)\n",
    "        data['bus'].delta_p = delta_p\n",
    "        data['bus'].delta_q = delta_q\n",
    "        return delta_s\n",
    "\n",
    "\n",
    "    def message_passing_update(self, data): \n",
    "        \"\"\" \"\"\"\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr \n",
    "        src, dst = edge_index \n",
    "\n",
    "        # Extract edge features\n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        b_ij = edge_attr[:, 3]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        line_ij = torch.stack([br_r, br_x, b_ij, tau_ij, shift_ij], dim=1)\n",
    "\n",
    "        # Get source node message vectors\n",
    "        m_src = data['bus'].m[src] \n",
    "\n",
    "        # Compute messages along edges\n",
    "        edge_input = torch.cat([m_src, line_ij], dim=1) \n",
    "        messages = self.phi(edge_input)  \n",
    "\n",
    "        # Aggregate messages to each destination node\n",
    "        num_nodes = data['bus'].pf_x.size(0)\n",
    "        agg_msg_i = torch.zeros((num_nodes, self.hidden_dim), device=messages.device)\n",
    "        agg_msg_i = agg_msg_i.index_add(0, dst, messages) \n",
    "\n",
    "        return agg_msg_i\n",
    "\n",
    "\n",
    "    def apply_nn_update(self, data, k): \n",
    "        \"\"\" \"\"\"\n",
    "        messages = self.message_passing_update(data)\n",
    "        self.layers[k](data, messages)\n",
    "\n",
    "\n",
    "class NNUpdate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L_theta = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_v = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_m = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data, messages):\n",
    "        \"\"\" \"\"\"\n",
    "        v = data['bus'].v.unsqueeze(1) \n",
    "        theta = data['bus'].theta.unsqueeze(1) \n",
    "        delta_p = data['bus'].delta_p.unsqueeze(1) \n",
    "        delta_q = data['bus'].delta_q.unsqueeze(1) \n",
    "        m = data['bus'].m\n",
    "        feature_vector = torch.cat([v, theta, delta_p, delta_q, m, messages], dim=1)\n",
    "\n",
    "        theta_update = self.L_theta(feature_vector)\n",
    "        v_update = self.L_v(feature_vector)\n",
    "        m_update = self.L_m(feature_vector)\n",
    "\n",
    "        # theta update \n",
    "        data['bus'].theta = data['bus'].theta + theta_update.squeeze(-1)\n",
    "        \n",
    "        # v only gets updated if it doesn't have a generator\n",
    "        _, gen_idx = data[('gen', 'gen_link', 'bus')].edge_index\n",
    "        v_update[gen_idx] = 0\n",
    "        data['bus'].v = data['bus'].v + v_update.squeeze(-1)\n",
    "\n",
    "        # m update \n",
    "        data['bus'].m = data['bus'].m + m_update\n",
    "\n",
    "\n",
    "class LocalPowerBalanceLoss:\n",
    "    \"\"\"\n",
    "    Compute the power balance loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.power_balance_loss = None\n",
    "    \n",
    "    def __call__(self, data, layer_loss=False, training=False):\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr\n",
    "        src, dst = edge_index\n",
    "\n",
    "        # Bus values\n",
    "        v = data['bus'].v\n",
    "        theta = data['bus'].theta\n",
    "        b_s = data['bus'].shunt[:, 1]\n",
    "        g_s = data['bus'].shunt[:, 0]\n",
    "        pg = data['bus'].pg\n",
    "        pd = data['bus'].pd\n",
    "        qd = data['bus'].qd\n",
    "        qg = data['bus'].qg\n",
    "\n",
    "        # Edge values\n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        b_ij = edge_attr[:, 3]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        y = 1 / (torch.complex(br_r, br_x))\n",
    "        y_ij = torch.abs(y)\n",
    "        delta_ij = torch.angle(y)\n",
    "\n",
    "        # Gather per-branch bus features\n",
    "        v_i = v[src]\n",
    "        v_j = v[dst]\n",
    "        theta_i = theta[src]\n",
    "        theta_j = theta[dst]\n",
    "\n",
    "        # Active power flows\n",
    "        P_flow_src = (\n",
    "            (v_i * v_j * y_ij / tau_ij) * torch.sin(theta_i - theta_j - delta_ij - shift_ij) \n",
    "            + ((v_i / tau_ij) ** 2) * (y_ij * torch.sin(delta_ij))\n",
    "        ) \n",
    "\n",
    "        P_flow_dst = (\n",
    "            (v_j * v_i * y_ij / tau_ij) * torch.sin(theta_j - theta_i - delta_ij + shift_ij)\n",
    "            + ((v_j) ** 2) * (y_ij * torch.sin(delta_ij))\n",
    "        )\n",
    "\n",
    "        # Reactive power flows\n",
    "        Q_flow_src = (\n",
    "            (-v_i * v_j * y_ij / tau_ij) * torch.cos(theta_i - theta_j - delta_ij - shift_ij)\n",
    "            + ((v_i / tau_ij) ** 2) * (y_ij * torch.cos(delta_ij) - b_ij / 2)\n",
    "        )\n",
    "\n",
    "        Q_flow_dst = (\n",
    "            (-v_j * v_i * y_ij / tau_ij) * torch.cos(theta_j - theta_i - delta_ij + shift_ij)\n",
    "            + ((v_j) ** 2) * (y_ij * torch.sin(delta_ij) - b_ij / 2)\n",
    "        ) \n",
    "\n",
    "        # Aggregate contributions for all nodes\n",
    "        Pbus_pred = torch.zeros_like(v).scatter_add_(0, src, P_flow_src)\n",
    "        Pbus_pred = Pbus_pred.scatter_add_(0, dst, P_flow_dst)\n",
    "        Qbus_pred = torch.zeros_like(v).scatter_add_(0, src, Q_flow_src)\n",
    "        Qbus_pred = Qbus_pred.scatter_add_(0, dst, Q_flow_dst)\n",
    "\n",
    "        if layer_loss: \n",
    "            delta_p = (pg - pd - g_s * (v ** 2)) + Pbus_pred\n",
    "            delta_q = qg - qd + b_s * (v ** 2) + Qbus_pred\n",
    "            delta_s = (delta_p ** 2 + delta_q ** 2).mean()\n",
    "            if training: \n",
    "                self.power_balance_loss = delta_s\n",
    "            return delta_p, delta_q, delta_s\n",
    "        else: \n",
    "            qg = qd - b_s * v**2 - Qbus_pred\n",
    "            return qg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1182edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphNeuralSolver(K=30, hidden_dim=10, gamma=0.1)\n",
    "device='cpu'\n",
    "case_14_data = PFDeltaGNS(root_dir='data/gns_data/case14_n/', case_name='case14', split='train')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = LocalPowerBalanceLoss()\n",
    "\n",
    "# Assume data_list is a list of HeteroData objects\n",
    "loader = DataLoader(case_14_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a32c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_196822/572474162.py\u001b[0m(21)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m        \u001b[0;31m# Send batch to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/abhagava/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'pf_x', 'delta_q', 'x_gns', 'bus_demand', 'bus_type', 'v', 'pd', 'qg', 'pf_y', 'bus_gen', 'bus_voltages', 'theta', 'pg', 'delta_p', 'shunt', 'qd'}'. Please explicitly set 'num_nodes' as an attribute of 'data[bus]' to suppress this warning\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Could not infer dtype of NoneType\n",
      "> \u001b[0;32m/tmp/ipykernel_196822/572474162.py\u001b[0m(21)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m        \u001b[0;31m# Send batch to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'batch' is not defined\n",
      "<tqdm.std.tqdm object at 0x7f220caac0d0>\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_196822/572474162.py\u001b[0m(21)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m        \u001b[0;31m# Send batch to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'batch' is not defined\n",
      "*** IndentationError: expected an indented block after 'for' statement on line 1\n"
     ]
    }
   ],
   "source": [
    "model = GraphNeuralSolver(K=30, hidden_dim=10, gamma=0.1)\n",
    "device='cpu'\n",
    "case_14_data = PFDeltaGNS(root_dir='data/gns_data/case14_n/', case_name='case14', split='train')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = LocalPowerBalanceLoss()\n",
    "\n",
    "# Assume data_list is a list of HeteroData objects\n",
    "loader = DataLoader(case_14_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    loader_iter = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    for batch in loader_iter:\n",
    "        # Send batch to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, layer_loss = model(batch)\n",
    "        _, _, final_loss = criterion(batch, layer_loss=True, training=True)\n",
    "        final_loss += layer_loss\n",
    "        loss = final_loss / batch_size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally update tqdm with current batch loss\n",
    "        loader_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Loss: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9b792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
