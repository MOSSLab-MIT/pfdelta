{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f33c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06f817",
   "metadata": {},
   "source": [
    "### Load Data and Preprocess for GNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7620622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.force_reload = force_reload\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()]) \n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        PQ_bus_x, PQ_bus_y = [], []\n",
    "        PV_bus_x, PV_bus_y = [], []\n",
    "        PV_demand, PV_generation = [], []\n",
    "        slack_x, slack_y = [], []\n",
    "        slack_demand, slack_generation = [], []\n",
    "        bus_x = []\n",
    "\n",
    "        PV_to_bus, PQ_to_bus, slack_to_bus = [], [], []\n",
    "        pq_idx, pv_idx, slack_idx = 0, 0, 0\n",
    "        gen_limits, gen_setpoints, more_gen_data  = [], [], []\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "\n",
    "            # Shunts \n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_x.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "                if int(gen['gen_bus']) == bus_id: \n",
    "                    if gen['gen_status'] == 1:\n",
    "                        gen_sol = solution_data['gen'][gen_id]\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            # Node features\n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            if bus['bus_type'] == 1:\n",
    "                PQ_bus_x.append(torch.tensor([pd, qd]))\n",
    "                PQ_bus_y.append(torch.tensor([va, vm]))\n",
    "                PQ_to_bus.append(torch.tensor([pq_idx, bus_idx]))\n",
    "                pq_idx += 1\n",
    "            elif bus['bus_type'] == 2:\n",
    "                PV_bus_x.append(torch.tensor([pg - pd, vm]))\n",
    "                PV_bus_y.append(torch.tensor([qg - qd, va]))\n",
    "                PV_demand.append(torch.tensor([pd, qd]))\n",
    "                PV_generation.append(torch.tensor([pg, qg]))\n",
    "                PV_to_bus.append(torch.tensor([pv_idx, bus_idx]))\n",
    "                pv_idx += 1\n",
    "            elif bus['bus_type'] == 3:\n",
    "                slack_x.append(torch.tensor([va, vm]))\n",
    "                slack_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                slack_demand.append(torch.tensor([pd, qd]))\n",
    "                slack_generation.append(torch.tensor([pg, qg]))\n",
    "                slack_to_bus.append(torch.tensor([slack_idx, bus_idx]))\n",
    "                slack_idx += 1\n",
    "        \n",
    "        # Generator\n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_sol = solution_data['gen'][gen_id]\n",
    "                pmin, pmax, qmin, qmax = gen['pmin'], gen['pmax'], gen['qmin'], gen['qmax']\n",
    "                pgen, qgen = gen_sol['pg'], gen_sol['qg']\n",
    "                gen_limits.append(torch.tensor([pmin, pmax, qmin, qmax]))\n",
    "                gen_setpoints.append(torch.tensor([pgen, qgen]))\n",
    "                is_slack = torch.tensor(\n",
    "                        1 if network_data['bus'][str(gen['gen_bus'])]['bus_type'] == 3 else 0,\n",
    "                        dtype=torch.bool\n",
    "                                )\n",
    "                gen_bus = torch.tensor(gen['gen_bus']) - 1  # zero-indexed\n",
    "                more_gen_data.append(torch.stack([gen_bus, is_slack]))\n",
    "            else:\n",
    "                assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "        # Edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            if branch['br_status'] == 0:\n",
    "                continue  # Skip inactive branches\n",
    "\n",
    "            from_bus = int(branch['f_bus']) - 1 \n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'], \n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            assert branch_sol is not None, f\"Missing solution for active branch {branch_id_str}\"\n",
    "\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['PQ'].x = torch.stack(PQ_bus_x) \n",
    "        data['PQ'].y = torch.stack(PQ_bus_y) \n",
    "\n",
    "        data['PV'].x = torch.stack(PV_bus_x) \n",
    "        data['PV'].y = torch.stack(PV_bus_y) \n",
    "        data['PV'].generation = torch.stack(PV_generation) \n",
    "        data['PV'].demand = torch.stack(PV_demand) \n",
    "\n",
    "        data['slack'].x = torch.stack(slack_x) \n",
    "        data['slack'].y = torch.stack(slack_y) \n",
    "        data['slack'].generation = torch.stack(slack_generation) \n",
    "        data['slack'].demand = torch.stack(slack_demand) \n",
    "\n",
    "        data['bus'].x = torch.stack(bus_x)\n",
    "\n",
    "        data['gen'].limits = torch.stack(gen_limits)\n",
    "        data['gen'].setpoints = torch.stack(gen_setpoints)\n",
    "        data['gen'].more_gen_data = torch.stack(more_gen_data)\n",
    "\n",
    "        data['bus', 'branch', 'bus'].edge_index = torch.stack(edge_index, dim=1) \n",
    "        data['bus', 'branch', 'bus'].edge_attr = torch.stack(edge_attr) \n",
    "        data['bus', 'branch', 'bus'].edge_label = torch.stack(edge_label) \n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('PV', 'PV_link', 'bus'): PV_to_bus,\n",
    "            ('PQ', 'PQ_link', 'bus'): PQ_to_bus,\n",
    "            ('slack', 'slack_link', 'bus'): slack_to_bus\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1) \n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):]\n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))\n",
    "\n",
    "\n",
    "\n",
    "class PFDeltaGNS(PFDeltaDataset): \n",
    "    def __init__(self, root_dir='data', case_name='', split='train', transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__(root_dir, case_name, split, transform, pre_transform, pre_filter, force_reload)\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        \"\"\" \"\"\"\n",
    "        # call base version\n",
    "        data = super().build_heterodata(pm_case)\n",
    "        num_buses = data['bus'].x.size(0)\n",
    "\n",
    "        # Init bus-level fields\n",
    "        v_buses      = torch.zeros(num_buses)\n",
    "        theta_buses  = torch.zeros(num_buses)\n",
    "        pd_buses     = torch.zeros(num_buses)\n",
    "        qd_buses     = torch.zeros(num_buses)\n",
    "        pg_buses     = torch.zeros(num_buses)\n",
    "        qg_buses     = torch.zeros(num_buses)\n",
    "\n",
    "        for ntype in ['PQ', 'PV', 'slack']:\n",
    "            node = data[ntype]\n",
    "            num_nodes = node.x.size(0)\n",
    "\n",
    "            if ntype == 'PQ':\n",
    "                # Flat start init\n",
    "                node.x_gns = torch.tensor([1.0, 0.0]).repeat(num_nodes, 1)\n",
    "                pd = node.x[:, 0]\n",
    "                qd = node.x[:, 1]\n",
    "                pg = torch.zeros_like(pd)\n",
    "                qg = torch.zeros_like(qd)\n",
    "\n",
    "            elif ntype == 'PV':\n",
    "                v = node.x[:, 1:2]\n",
    "                theta = torch.zeros_like(v)\n",
    "                node.x_gns = torch.cat([v, theta], dim=1)\n",
    "                pd = node.demand[:, 0]\n",
    "                qd = node.demand[:, 1]\n",
    "                pg = node.generation[:, 0]\n",
    "                qg = node.generation[:, 1]\n",
    "\n",
    "            elif ntype == 'slack':\n",
    "                node.x_gns = node.x.clone()\n",
    "                pd = node.demand[:, 0]\n",
    "                qd = node.demand[:, 1]\n",
    "                pg = node.generation[:, 0]\n",
    "                qg = node.generation[:, 1]\n",
    "\n",
    "            # Map to buses\n",
    "            link_type = ntype + '_link'\n",
    "            edge_index = data[(ntype, link_type, 'bus')].edge_index\n",
    "            src, dst = edge_index  # src is local node index, dst is bus index\n",
    "\n",
    "            x_gns = node.x_gns\n",
    "            v_dst = x_gns[:, 0]\n",
    "            theta_dst = x_gns[:, 1]\n",
    "\n",
    "            v_buses.index_add_(0, dst, v_dst)\n",
    "            theta_buses.index_add_(0, dst, theta_dst)\n",
    "            pd_buses.index_add_(0, dst, pd)\n",
    "            qd_buses.index_add_(0, dst, qd)\n",
    "            pg_buses.index_add_(0, dst, pg)\n",
    "            qg_buses.index_add_(0, dst, qg)\n",
    "\n",
    "        # Store in bus\n",
    "        data['bus'].v = v_buses\n",
    "        data['bus'].theta = theta_buses\n",
    "        data['bus'].pd = pd_buses\n",
    "        data['bus'].qd = qd_buses\n",
    "        data['bus'].pg = pg_buses\n",
    "        data['bus'].qg = qg_buses\n",
    "        data['bus'].delta_p = torch.zeros_like(v_buses)\n",
    "        data['bus'].delta_q = torch.zeros_like(v_buses)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b7f38",
   "metadata": {},
   "source": [
    "### Graph Neural Solver Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42dee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphNeuralSolver(nn.Module):\n",
    "    def __init__(self, K, hidden_dim, gamma):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.gamma = gamma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.phi_input_dim = hidden_dim + 5\n",
    "        self.L_input_dim = 2 * hidden_dim + 4\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(self.phi_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            NNUpdate(self.L_input_dim, hidden_dim) for _ in range(K)\n",
    "        )\n",
    "        self.power_balance = LocalPowerBalanceLoss()\n",
    "\n",
    "    def forward(self, data):  \n",
    "        \"\"\" \"\"\"\n",
    "        device = data['bus'].x.device \n",
    "\n",
    "        # Instantiate message vectors for each bus\n",
    "        num_nodes = data['bus'].x.size(0)\n",
    "        data['bus'].m = torch.zeros((num_nodes, self.hidden_dim), device=device)\n",
    "        total_layer_loss = 0\n",
    "\n",
    "        for k in range(self.K): \n",
    "\n",
    "            # Update P and Q values for all buses\n",
    "            self.global_active_compensation(data) \n",
    "\n",
    "            # Compute local power imbalance variables and store power imbalance loss \n",
    "            layer_loss = self.local_power_imbalance(data, layer_loss=True)\n",
    "            total_layer_loss += layer_loss * (self.gamma ** (self.K - k))\n",
    "\n",
    "            # Apply the neural network update block \n",
    "            self.apply_nn_update(data, k)\n",
    "        \n",
    "        return data, total_layer_loss\n",
    "\n",
    "\n",
    "    def global_active_compensation(self, data):\n",
    "        \"\"\" \"\"\"\n",
    "        # Compute global power demand \n",
    "        p_joule = self.compute_p_joule(data)   \n",
    "        p_global = self.compute_p_global(data, p_joule)\n",
    "\n",
    "        # Compute pg_slack and assign to relevant buses\n",
    "        pg_slack = self.compute_pg_slack(p_global, data)\n",
    "        edge_index = data[('slack', 'slack_link', 'bus')].edge_index\n",
    "        _, dst = edge_index \n",
    "        data['bus'].pg[dst] = pg_slack\n",
    "\n",
    "        # Compute qg values for each bus\n",
    "        qg = self.power_balance(data, layer_loss=False)\n",
    "        data['bus'].qg = qg\n",
    "\n",
    "\n",
    "    def compute_p_joule(self, data): \n",
    "        \"\"\" \"\"\"\n",
    "        # Extract edge index and attributes\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr \n",
    "        src, dst = edge_index \n",
    "\n",
    "        # Edge features\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "\n",
    "        # Line admittance features \n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        y = 1 / (torch.complex(br_r, br_x))\n",
    "        if torch.any((torch.complex(br_r, br_x)) == 0):\n",
    "            print(\"Division by zero detected!\")\n",
    "        y_ij = torch.abs(y)\n",
    "        delta_ij = torch.angle(y)\n",
    "\n",
    "        # Node features\n",
    "        v_i = data['bus'].v[src]\n",
    "        v_j = data['bus'].v[dst]\n",
    "        theta_i = data['bus'].theta[src]\n",
    "        theta_j = data['bus'].theta[dst]\n",
    "\n",
    "        # Compute p_global\n",
    "        term1 = v_i * v_j * y_ij / tau_ij * (\n",
    "            torch.sin(theta_i - theta_j - delta_ij - shift_ij) +  \n",
    "            torch.sin(theta_j - theta_i - delta_ij + shift_ij))\n",
    "\n",
    "        term2 = (v_i / tau_ij) ** 2 * y_ij * torch.sin(delta_ij)\n",
    "        term3 = v_j ** 2 * y_ij * torch.sin(delta_ij)\n",
    "        p_joule_edge = torch.abs(term1 + term2 + term3)\n",
    "        \n",
    "        # Map to individual graphs\n",
    "        bus_batch = data['bus'].batch  # batch index per bus\n",
    "        edge_batch = bus_batch[src]    # batch index per edge (via source bus)\n",
    "        num_graphs = int(edge_batch.max()) + 1\n",
    "        p_joule_per_graph = torch.zeros(num_graphs, device=p_joule_edge.device)\n",
    "        p_joule_per_graph = p_joule_per_graph.index_add(0, edge_batch, p_joule_edge)\n",
    "\n",
    "        return p_joule_per_graph\n",
    "    \n",
    "\n",
    "    def compute_p_global(self, data, p_joule): \n",
    "        \"\"\" \"\"\"\n",
    "        # Per-bus data\n",
    "        pd = data['bus'].pd \n",
    "        v = data['bus'].v\n",
    "        g_shunt = data['bus'].x[:, 0]\n",
    "\n",
    "        # Graph assignment per bus\n",
    "        bus_batch = data['bus'].batch\n",
    "        num_graphs = int(bus_batch.max()) + 1\n",
    "\n",
    "        # Compute local p_global components\n",
    "        p_global_local = pd + (v ** 2) * g_shunt\n",
    "\n",
    "        # Sum per graph\n",
    "        p_global = torch.zeros(num_graphs, device=p_global_local.device)\n",
    "        p_global = p_global.index_add(0, bus_batch, p_global_local)\n",
    "\n",
    "        # Add per-graph Joule losses\n",
    "        p_global += p_joule\n",
    "\n",
    "        return p_global\n",
    "\n",
    "\n",
    "    def compute_pg_slack(self, p_global, data):\n",
    "        \"\"\" \"\"\"\n",
    "        pg_setpoints = data['gen'].setpoints[:, 0] \n",
    "        pg_max_vals = data['gen'].limits[:, 1]\n",
    "        pg_min_vals = data['gen'].limits[:, 0]\n",
    "        is_slack = data['gen'].more_gen_data[:, 1] == 1 \n",
    "        graph_ids = is_slack.cumsum(dim=0) - 1\n",
    "        num_graphs = graph_ids.max().item() + 1\n",
    "        pg_setpoint_slack = pg_setpoints[is_slack]\n",
    "        pg_setpoints_non_slack = pg_setpoints.clone()\n",
    "        pg_setpoints_non_slack[is_slack] = 0.0\n",
    "        pg_max_slack = torch.full_like(pg_max_vals[is_slack], 3.4)\n",
    "        pg_min_slack = torch.full_like(pg_min_vals[is_slack], 0.0)\n",
    "        # TODO: ADD THIS BACK LATER pg_max_vals[is_slack] \n",
    "        # TODO: ADD THIS BACK LATER pg_min_vals[is_slack]  \n",
    "\n",
    "        # Sum of total generator setpoints per graph\n",
    "        pg_setpoints_sum = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        pg_setpoints_sum = pg_setpoints_sum.index_add(0, graph_ids, pg_setpoints)\n",
    "        pg_non_slack_setpoints_sum = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        pg_non_slack_setpoints_sum = pg_non_slack_setpoints_sum.index_add(0, graph_ids, pg_setpoints_non_slack)\n",
    "\n",
    "        # Compute lambda in a vectorized way\n",
    "        under = (p_global < pg_setpoints_sum)\n",
    "        over = ~under\n",
    "        lamb = torch.zeros(num_graphs, device=pg_setpoints.device)\n",
    "        lamb[under] = (\n",
    "            (p_global[under] - pg_non_slack_setpoints_sum[under] - pg_max_slack[under]) /\n",
    "            (2 * (pg_setpoint_slack[under] - pg_min_slack[under]))\n",
    "        )\n",
    "        lamb[over] = (\n",
    "            (p_global[over] - pg_non_slack_setpoints_sum[over] - 2 * pg_setpoint_slack[over] - pg_max_slack[over]) /\n",
    "            (2 * (pg_max_slack[over] - pg_setpoint_slack[over]))\n",
    "        )\n",
    "\n",
    "        lamb = torch.clamp(lamb, min=0.0)\n",
    "        pg_slack = torch.zeros_like(lamb)\n",
    "\n",
    "        # Compute the pg_slack values \n",
    "        case1 = lamb < 0.5\n",
    "        case2 = ~case1\n",
    "        pg_slack[case1] = pg_min_slack[case1] + 2 * (pg_setpoint_slack[case1] - pg_min_slack[case1]) * lamb[case1]\n",
    "\n",
    "        pg_slack[case2] = (\n",
    "            2 * pg_setpoint_slack[case2] - pg_max_slack[case2] +\n",
    "            2 * (pg_max_slack[case2] - pg_setpoint_slack[case2]) * lamb[case2]\n",
    "        )\n",
    "        \n",
    "        return pg_slack\n",
    "\n",
    "\n",
    "    def local_power_imbalance(self, data, layer_loss):\n",
    "        \"\"\" \"\"\"\n",
    "        delta_p, delta_q, delta_s = self.power_balance(data, layer_loss)\n",
    "        data['bus'].delta_p = delta_p\n",
    "        data['bus'].delta_q = delta_q\n",
    "        return delta_s\n",
    "\n",
    "\n",
    "    def message_passing_update(self, data): \n",
    "        \"\"\" \"\"\"\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr \n",
    "        src, dst = edge_index \n",
    "\n",
    "        # Extract edge features\n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        b_ij = edge_attr[:, 3]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        line_ij = torch.stack([br_r, br_x, b_ij, tau_ij, shift_ij], dim=1)\n",
    "\n",
    "        # Get source node message vectors\n",
    "        m_src = data['bus'].m[src] \n",
    "\n",
    "        # Compute messages along edges\n",
    "        edge_input = torch.cat([m_src, line_ij], dim=1) \n",
    "        messages = self.phi(edge_input)  \n",
    "\n",
    "        # Aggregate messages to each destination node\n",
    "        num_nodes = data['bus'].x.size(0)\n",
    "        agg_msg_i = torch.zeros((num_nodes, self.hidden_dim), device=messages.device)\n",
    "        agg_msg_i = agg_msg_i.index_add(0, dst, messages) \n",
    "\n",
    "        return agg_msg_i\n",
    "\n",
    "\n",
    "    def apply_nn_update(self, data, k): \n",
    "        \"\"\" \"\"\"\n",
    "        messages = self.message_passing_update(data)\n",
    "        self.layers[k](data, messages)\n",
    "\n",
    "\n",
    "\n",
    "class NNUpdate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L_theta = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_v = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.L_m = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data, messages):\n",
    "        \"\"\" \"\"\"\n",
    "        v = data['bus'].v.unsqueeze(1) \n",
    "        theta = data['bus'].theta.unsqueeze(1) \n",
    "        delta_p = data['bus'].delta_p.unsqueeze(1) \n",
    "        delta_q = data['bus'].delta_q.unsqueeze(1) \n",
    "        m = data['bus'].m\n",
    "        feature_vector = torch.cat([v, theta, delta_p, delta_q, m, messages], dim=1)\n",
    "\n",
    "        theta_update = self.L_theta(feature_vector)\n",
    "        v_update = self.L_v(feature_vector)\n",
    "        m_update = self.L_m(feature_vector)\n",
    "\n",
    "        # theta update \n",
    "        data['bus'].theta = data['bus'].theta + theta_update.squeeze(-1)\n",
    "        \n",
    "        # v only gets updated if it doesn't have a generator\n",
    "        _, gen_idx = data[('PV', 'PV_link', 'bus')].edge_index\n",
    "        v_update[gen_idx] = 0\n",
    "        data['bus'].v = data['bus'].v + v_update.squeeze(-1)\n",
    "\n",
    "        # m update \n",
    "        data['bus'].m = data['bus'].m + m_update\n",
    "\n",
    "\n",
    "\n",
    "class LocalPowerBalanceLoss:\n",
    "    \"\"\"\n",
    "    Compute the power balance loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.power_balance_loss = None\n",
    "    \n",
    "    def __call__(self, data, layer_loss=False, training=False):\n",
    "        edge_index = data[('bus', 'branch', 'bus')].edge_index\n",
    "        edge_attr = data[('bus', 'branch', 'bus')].edge_attr\n",
    "        src, dst = edge_index\n",
    "\n",
    "        # Bus values\n",
    "        v = data['bus'].v\n",
    "        theta = data['bus'].theta\n",
    "        b_s = data['bus'].x[:,1]\n",
    "        g_s = data['bus'].x[:,0]\n",
    "        pg = data['bus'].pg\n",
    "        pd = data['bus'].pd\n",
    "        qd = data['bus'].qd\n",
    "        qg = data['bus'].qg\n",
    "\n",
    "        # Edge values\n",
    "        br_r = edge_attr[:, 0]\n",
    "        br_x = edge_attr[:, 1]\n",
    "        b_ij = edge_attr[:, 3]\n",
    "        shift_ij = edge_attr[:, -1]\n",
    "        tau_ij = edge_attr[:, -2]\n",
    "        y = 1 / (torch.complex(br_r, br_x))\n",
    "        y_ij = torch.abs(y)\n",
    "        delta_ij = torch.angle(y)\n",
    "\n",
    "        # Gather per-branch bus features\n",
    "        v_i = v[src]\n",
    "        v_j = v[dst]\n",
    "        theta_i = theta[src]\n",
    "        theta_j = theta[dst]\n",
    "\n",
    "        # Active power flows\n",
    "        P_flow_src = (\n",
    "            (v_i * v_j * y_ij / tau_ij) * torch.sin(theta_i - theta_j - delta_ij - shift_ij) \n",
    "            + ((v_i / tau_ij) ** 2) * (y_ij * torch.sin(delta_ij))\n",
    "        ) \n",
    "\n",
    "        P_flow_dst = (\n",
    "            (v_j * v_i * y_ij / tau_ij) * torch.sin(theta_j - theta_i - delta_ij + shift_ij)\n",
    "            + ((v_j) ** 2) * (y_ij * torch.sin(delta_ij))\n",
    "        )\n",
    "\n",
    "        # Reactive power flows\n",
    "        Q_flow_src = (\n",
    "            (-v_i * v_j * y_ij / tau_ij) * torch.cos(theta_i - theta_j - delta_ij - shift_ij)\n",
    "            + ((v_i / tau_ij) ** 2) * (y_ij * torch.cos(delta_ij) - b_ij / 2)\n",
    "        )\n",
    "\n",
    "        Q_flow_dst = (\n",
    "            (-v_j * v_i * y_ij / tau_ij) * torch.cos(theta_j - theta_i - delta_ij + shift_ij)\n",
    "            + ((v_j) ** 2) * (y_ij * torch.sin(delta_ij) - b_ij / 2)\n",
    "        ) \n",
    "\n",
    "        # Aggregate contributions for all nodes\n",
    "        Pbus_pred = torch.zeros_like(v).scatter_add_(0, src, P_flow_src)\n",
    "        Pbus_pred = Pbus_pred.scatter_add_(0, dst, P_flow_dst)\n",
    "        Qbus_pred = torch.zeros_like(v).scatter_add_(0, src, Q_flow_src)\n",
    "        Qbus_pred = Qbus_pred.scatter_add_(0, dst, Q_flow_dst)\n",
    "\n",
    "        if layer_loss: \n",
    "            delta_p = (pg - pd - g_s * (v ** 2)) + Pbus_pred\n",
    "            delta_q = qg - qd + b_s * (v ** 2) + Qbus_pred\n",
    "            delta_s = (delta_p ** 2 + delta_q ** 2).mean()\n",
    "            if training: \n",
    "                self.power_balance_loss = delta_s\n",
    "            return delta_p, delta_q, delta_s\n",
    "        else: \n",
    "            qg = qd - b_s * v**2 - Qbus_pred\n",
    "            return qg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0a32c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m _, layer_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m _, _, final_loss \u001b[38;5;241m=\u001b[39m criterion(batch, layer_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m final_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m layer_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mGraphNeuralSolver.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m total_layer_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK): \n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Update P and Q values for all buses\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_active_compensation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Compute local power imbalance variables and store power imbalance loss \u001b[39;00m\n\u001b[1;32m     35\u001b[0m     layer_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_power_imbalance(data, layer_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m, in \u001b[0;36mGraphNeuralSolver.global_active_compensation\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     54\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpg[dst] \u001b[38;5;241m=\u001b[39m pg_slack\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Compute qg values for each bus\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m qg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower_balance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mqg \u001b[38;5;241m=\u001b[39m qg\n",
      "Cell \u001b[0;32mIn[4], line 314\u001b[0m, in \u001b[0;36mLocalPowerBalanceLoss.__call__\u001b[0;34m(self, data, layer_loss, training)\u001b[0m\n\u001b[1;32m    310\u001b[0m theta_j \u001b[38;5;241m=\u001b[39m theta[dst]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Active power flows\u001b[39;00m\n\u001b[1;32m    313\u001b[0m P_flow_src \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 314\u001b[0m     (v_i \u001b[38;5;241m*\u001b[39m v_j \u001b[38;5;241m*\u001b[39m y_ij \u001b[38;5;241m/\u001b[39m tau_ij) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta_j\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_ij\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshift_ij\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    315\u001b[0m     \u001b[38;5;241m+\u001b[39m ((v_i \u001b[38;5;241m/\u001b[39m tau_ij) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (y_ij \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(delta_ij))\n\u001b[1;32m    316\u001b[0m ) \n\u001b[1;32m    318\u001b[0m P_flow_dst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m     (v_j \u001b[38;5;241m*\u001b[39m v_i \u001b[38;5;241m*\u001b[39m y_ij \u001b[38;5;241m/\u001b[39m tau_ij) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(theta_j \u001b[38;5;241m-\u001b[39m theta_i \u001b[38;5;241m-\u001b[39m delta_ij \u001b[38;5;241m+\u001b[39m shift_ij)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;241m+\u001b[39m ((v_j) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (y_ij \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(delta_ij))\n\u001b[1;32m    321\u001b[0m )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Reactive power flows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GraphNeuralSolver(K=30, hidden_dim=10, gamma=0.1)\n",
    "device='cpu'\n",
    "case_14_data = PFDeltaGNS(root_dir='data/gns_data', case_name='case14', split='train')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = LocalPowerBalanceLoss()\n",
    "\n",
    "# Assume data_list is a list of HeteroData objects\n",
    "loader = DataLoader(case_14_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    loader_iter = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "    for batch in loader_iter:\n",
    "        # Send batch to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, layer_loss = model(batch)\n",
    "        _, _, final_loss = criterion(batch, layer_loss=True, training=True)\n",
    "        final_loss += layer_loss\n",
    "        loss = final_loss / batch_size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally update tqdm with current batch loss\n",
    "        loader_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Loss: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205bf964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-04-26 16:21:59 3240688:3240688 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2025-04-26 16:21:59 3240688:3240688 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2025-04-26 16:21:59 3240688:3240688 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::index        14.42%       4.605ms        20.95%       6.691ms      36.168us           185  \n",
      "                    aten::mul         9.38%       2.994ms        11.22%       3.584ms      11.378us           315  \n",
      "                 aten::linear         0.55%     175.000us        10.02%       3.199ms      79.975us            40  \n",
      "                  aten::addmm         5.94%       1.897ms         8.03%       2.565ms      64.125us            40  \n",
      "                    aten::sub         5.56%       1.776ms         5.56%       1.776ms       7.893us           225  \n",
      "                 aten::select         3.88%       1.238ms         4.65%       1.484ms       4.066us           365  \n",
      "                    aten::abs         1.85%     591.000us         4.59%       1.465ms      36.625us            40  \n",
      "                aten::nonzero         3.39%       1.082ms         4.41%       1.407ms      10.422us           135  \n",
      "             aten::layer_norm         0.33%     105.000us         4.28%       1.367ms      68.350us            20  \n",
      "      aten::native_layer_norm         3.60%       1.149ms         4.16%       1.327ms      66.350us            20  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 31.934ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile() as profiler:\n",
    "    _, layer_loss = model(batch)\n",
    "\n",
    "print(profiler.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
