{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2474a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import InMemoryDataset, HeteroData\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b60f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the architectures into main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0730198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir='data', case_name='', split='train', add_bus_type=False, transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        self.split = split\n",
    "        self.case_name = case_name\n",
    "        self.force_reload = force_reload\n",
    "        self.add_bus_type = add_bus_type\n",
    "        root = os.path.join(root_dir, case_name)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[self._split_to_idx()]) \n",
    "\n",
    "    def _split_to_idx(self):\n",
    "        return {'train': 0, 'val': 1, 'test': 2, 'all': 3}[self.split]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted([f for f in os.listdir(self.raw_dir) if f.endswith('.json')])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt', 'all.pt']\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        data = HeteroData()\n",
    "\n",
    "        network_data = pm_case['network']\n",
    "        solution_data = pm_case['solution']['solution']\n",
    "\n",
    "        # Bus nodes\n",
    "        pf_x, pf_y = [], []\n",
    "        bus_voltages = []\n",
    "        bus_type = []\n",
    "        bus_shunts = []\n",
    "        bus_gen, bus_demand = [], []\n",
    "\n",
    "        PQ_bus_x, PQ_bus_y = [], []\n",
    "        PV_bus_x, PV_bus_y = [], []\n",
    "        PV_demand, PV_generation = [], []\n",
    "        slack_x, slack_y = [], []\n",
    "        slack_demand, slack_generation = [], []\n",
    "        PV_to_bus, PQ_to_bus, slack_to_bus = [], [], []\n",
    "        pq_idx, pv_idx, slack_idx = 0, 0, 0\n",
    "\n",
    "        for bus_id_str, bus in sorted(network_data['bus'].items(), key=lambda x: int(x[0])):\n",
    "            bus_id = int(bus_id_str)\n",
    "            bus_idx = bus_id - 1\n",
    "            bus_sol = solution_data['bus'][bus_id_str]\n",
    "            \n",
    "            va, vm = bus_sol['va'], bus_sol['vm']\n",
    "            bus_voltages.append(torch.tensor([va, vm]))\n",
    "\n",
    "            # Shunts \n",
    "            gs, bs = 0.0, 0.0\n",
    "            for shunt in network_data['shunt'].values():\n",
    "                if int(shunt['shunt_bus']) == bus_id:\n",
    "                    gs += shunt['gs']\n",
    "                    bs += shunt['bs']\n",
    "            bus_shunts.append(torch.tensor([gs, bs]))\n",
    "\n",
    "            # Load\n",
    "            pd, qd = 0.0, 0.0\n",
    "            for load in network_data['load'].values():\n",
    "                if int(load['load_bus']) == bus_id:\n",
    "                    pd += load['pd']\n",
    "                    qd += load['qd']\n",
    "\n",
    "            bus_demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "            # Gen\n",
    "            pg, qg = 0.0, 0.0\n",
    "            for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "                if int(gen['gen_bus']) == bus_id: \n",
    "                    if gen['gen_status'] == 1:\n",
    "                        gen_sol = solution_data['gen'][gen_id]\n",
    "                        pg += gen_sol['pg']\n",
    "                        qg += gen_sol['qg']\n",
    "                    else:\n",
    "                        assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "            bus_gen.append(torch.tensor([pg, qg]))\n",
    "\n",
    "            # Now decide final bus type\n",
    "            bus_type_now = bus['bus_type']\n",
    "\n",
    "            if bus_type_now == 2 and pg == 0.0 and qg == 0.0:\n",
    "                bus_type_now = 1  # PV bus with no gen --> becomes PQ\n",
    "                # maybe add an assert here to check if all gens were off.\n",
    "\n",
    "            bus_type.append(torch.tensor(bus_type_now))\n",
    "\n",
    "            if bus_type_now == 1:\n",
    "                pf_x.append(torch.tensor([pd, qd]))\n",
    "                pf_y.append(torch.tensor([va, vm]))\n",
    "\n",
    "                PQ_bus_x.append(torch.tensor([pd, qd]))\n",
    "                PQ_bus_y.append(torch.tensor([va, vm]))\n",
    "                PQ_to_bus.append(torch.tensor([pq_idx, bus_idx]))\n",
    "                pq_idx += 1\n",
    "            elif bus_type_now == 2:\n",
    "                pf_x.append(torch.tensor([pg - pd, vm]))\n",
    "                pf_y.append(torch.tensor([qg - qd, va]))\n",
    "                \n",
    "                PV_bus_x.append(torch.tensor([pg - pd, vm]))\n",
    "                PV_bus_y.append(torch.tensor([qg - qd, va]))\n",
    "                PV_demand.append(torch.tensor([pd, qd]))\n",
    "                PV_generation.append(torch.tensor([pg, qg]))\n",
    "                PV_to_bus.append(torch.tensor([pv_idx, bus_idx]))\n",
    "                pv_idx += 1\n",
    "            elif bus_type_now == 3:\n",
    "                pf_x.append(torch.tensor([va, vm]))\n",
    "                pf_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "\n",
    "                slack_x.append(torch.tensor([va, vm]))\n",
    "                slack_y.append(torch.tensor([pg - pd, qg - qd]))\n",
    "                slack_demand.append(torch.tensor([pd, qd]))\n",
    "                slack_generation.append(torch.tensor([pg, qg]))\n",
    "                slack_to_bus.append(torch.tensor([slack_idx, bus_idx]))\n",
    "                slack_idx += 1\n",
    "\n",
    "        generation, limits, slack_gen  = [], [], []\n",
    "\n",
    "        # Generator nodes   \n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_sol = solution_data['gen'][gen_id] \n",
    "                pmin, pmax, qmin, qmax = gen['pmin'], gen['pmax'], gen['qmin'], gen['qmax']\n",
    "                pgen, qgen = gen_sol['pg'], gen_sol['qg']\n",
    "                limits.append(torch.tensor([pmin, pmax, qmin, qmax]))\n",
    "                generation.append(torch.tensor([pgen, qgen]))\n",
    "                is_slack = torch.tensor(\n",
    "                        1 if network_data['bus'][str(gen['gen_bus'])]['bus_type'] == 3 else 0,\n",
    "                        dtype=torch.bool\n",
    "                                )\n",
    "                slack_gen.append(is_slack)\n",
    "            else:\n",
    "                assert solution_data['gen'].get(gen_id) is None, f\"Expected gen {gen_id} to be off.\"\n",
    "\n",
    "        # Load nodes\n",
    "        demand = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            pd, qd = load['pd'], load['qd']\n",
    "            demand.append(torch.tensor([pd, qd]))\n",
    "\n",
    "        # Edges\n",
    "        # bus to bus edges\n",
    "        edge_index, edge_attr, edge_label = [], [], []\n",
    "        for branch_id_str, branch in sorted(network_data['branch'].items(), key=lambda x: int(x[0])):\n",
    "            if branch['br_status'] == 0:\n",
    "                continue  # Skip inactive branches\n",
    "\n",
    "            from_bus = int(branch['f_bus']) - 1 \n",
    "            to_bus = int(branch['t_bus']) - 1\n",
    "            edge_index.append(torch.tensor([from_bus, to_bus]))\n",
    "            edge_attr.append(torch.tensor([\n",
    "                branch['br_r'], branch['br_x'],\n",
    "                branch['g_fr'], branch['b_fr'],\n",
    "                branch['g_to'], branch['b_to'], \n",
    "                branch['tap'],  branch['shift']\n",
    "            ]))\n",
    "\n",
    "            branch_sol = solution_data['branch'].get(branch_id_str)\n",
    "            assert branch_sol is not None, f\"Missing solution for active branch {branch_id_str}\"\n",
    "\n",
    "            if branch_sol:\n",
    "                edge_label.append(torch.tensor([\n",
    "                    branch_sol['pf'], branch_sol['qf'],\n",
    "                    branch_sol['pt'], branch_sol['qt']\n",
    "                ]))\n",
    "\n",
    "        # bus to gen edges\n",
    "        gen_to_bus_index = []\n",
    "        for gen_id, gen in sorted(network_data['gen'].items(), key=lambda x: int(x[0])):\n",
    "            if gen['gen_status'] == 1:\n",
    "                gen_bus = torch.tensor(gen['gen_bus']) - 1\n",
    "                gen_to_bus_index.append(torch.tensor([int(gen_id) - 1, gen_bus]))\n",
    "\n",
    "        # bus to load edges\n",
    "        load_to_bus_index = []\n",
    "        for load_id, load in sorted(network_data['load'].items(), key=lambda x: int(x[0])):\n",
    "            load_bus = torch.tensor(load['load_bus']) - 1\n",
    "            load_to_bus_index.append(torch.tensor([int(load_id) - 1, load_bus]))\n",
    "\n",
    "        # Create graph nodes and edges\n",
    "        data['bus'].x = torch.stack(pf_x)\n",
    "        data['bus'].y = torch.stack(pf_y)\n",
    "        data['bus'].bus_gen = torch.stack(bus_gen) # aggregated\n",
    "        data['bus'].bus_demand = torch.stack(bus_demand) # aggregated\n",
    "        data['bus'].bus_voltages = torch.stack(bus_voltages)\n",
    "        data['bus'].bus_type = torch.stack(bus_type)\n",
    "        data['bus'].shunt = torch.stack(bus_shunts)\n",
    "\n",
    "        data['gen'].limits = torch.stack(limits)\n",
    "        data['gen'].generation = torch.stack(generation)\n",
    "        data['gen'].slack_gen = torch.stack(slack_gen)\n",
    "\n",
    "        data['load'].demand = torch.stack(demand)\n",
    "\n",
    "        if self.add_bus_type:\n",
    "            data['PQ'].x = torch.stack(PQ_bus_x) \n",
    "            data['PQ'].y = torch.stack(PQ_bus_y)\n",
    "\n",
    "            data['PV'].x = torch.stack(PV_bus_x) \n",
    "            data['PV'].y = torch.stack(PV_bus_y) \n",
    "            data['PV'].generation = torch.stack(PV_generation) \n",
    "            data['PV'].demand = torch.stack(PV_demand) \n",
    "\n",
    "            data['slack'].x = torch.stack(slack_x) \n",
    "            data['slack'].y = torch.stack(slack_y)\n",
    "            data['slack'].generation = torch.stack(slack_generation) \n",
    "            data['slack'].demand = torch.stack(slack_demand)         \n",
    "\n",
    "        for link_name, edges in {\n",
    "            ('bus', 'branch', 'bus'): edge_index,\n",
    "            ('gen', 'gen_link', 'bus'): gen_to_bus_index,\n",
    "            ('load', 'load_link', 'bus'): load_to_bus_index\n",
    "        }.items():\n",
    "            edge_tensor = torch.stack(edges, dim=1) \n",
    "            data[link_name].edge_index = edge_tensor\n",
    "            data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "            if link_name == ('bus', 'branch', 'bus'): \n",
    "                data[link_name].edge_attr = torch.stack(edge_attr) \n",
    "        \n",
    "        if self.add_bus_type:\n",
    "            for link_name, edges in {\n",
    "                ('PV', 'PV_link', 'bus'): PV_to_bus,\n",
    "                ('PQ', 'PQ_link', 'bus'): PQ_to_bus,\n",
    "                ('slack', 'slack_link', 'bus'): slack_to_bus\n",
    "            }.items():\n",
    "                edge_tensor = torch.stack(edges, dim=1) \n",
    "                data[link_name].edge_index = edge_tensor\n",
    "                data[(link_name[2], link_name[1], link_name[0])].edge_index = edge_tensor.flip(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = self.raw_file_names\n",
    "        random.shuffle(fnames)\n",
    "        n = len(fnames)\n",
    "\n",
    "        split_dict = {\n",
    "            'train': fnames[:int(0.8 * n)],\n",
    "            'val': fnames[int(0.8 * n): int(0.9 * n)],\n",
    "            'test': fnames[int(0.9 * n):],\n",
    "            'all': fnames  # 👈 this uses all samples\n",
    "        }\n",
    "\n",
    "        for split, files in split_dict.items():\n",
    "            data_list = []\n",
    "            print(f\"Processing split: {split} ({len(files)} files)\")\n",
    "            for fname in tqdm(files, desc=f\"Building {split} data\"):\n",
    "                with open(os.path.join(self.raw_dir, fname)) as f:\n",
    "                    pm_case = json.load(f)\n",
    "                data = self.build_heterodata(pm_case)\n",
    "                data_list.append(data)\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), os.path.join(self.processed_dir, f'{split}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bca46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaCANOS(PFDeltaDataset): \n",
    "    def __init__(self, root_dir='data', case_name='', split='train', add_bus_type=False, transform=None, pre_transform=None, pre_filter=None, force_reload=False):\n",
    "        super().__init__(root_dir, case_name, split, add_bus_type, transform, pre_transform, pre_filter, force_reload)\n",
    "\n",
    "    def build_heterodata(self, pm_case):\n",
    "        # call base version\n",
    "        data = super().build_heterodata(pm_case)\n",
    "\n",
    "        # Now prune the data to only keep bus, PV, PQ, slack\n",
    "        keep_nodes = {\"bus\", \"PV\", \"PQ\", \"slack\"}\n",
    "\n",
    "        for node_type in list(data.node_types):\n",
    "            if node_type not in keep_nodes:\n",
    "                del data[node_type]\n",
    "\n",
    "        for edge_type in list(data.edge_types):\n",
    "            src, _, dst = edge_type\n",
    "            if src not in keep_nodes or dst not in keep_nodes:\n",
    "                del data[edge_type]\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_14_data = PFDeltaCANOS(root_dir='data/gns_data/', add_bus_type=True, case_name='case14', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a91d625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  bus={\n",
       "    x=[14, 2],\n",
       "    y=[14, 2],\n",
       "    bus_gen=[14, 2],\n",
       "    bus_demand=[14, 2],\n",
       "    bus_voltages=[14, 2],\n",
       "    bus_type=[14],\n",
       "    shunt=[14, 2],\n",
       "  },\n",
       "  PQ={\n",
       "    x=[9, 2],\n",
       "    y=[9, 2],\n",
       "  },\n",
       "  PV={\n",
       "    x=[4, 2],\n",
       "    y=[4, 2],\n",
       "    generation=[4, 2],\n",
       "    demand=[4, 2],\n",
       "  },\n",
       "  slack={\n",
       "    x=[1, 2],\n",
       "    y=[1, 2],\n",
       "    generation=[1, 2],\n",
       "    demand=[1, 2],\n",
       "  },\n",
       "  (bus, branch, bus)={\n",
       "    edge_index=[2, 20],\n",
       "    edge_attr=[20, 8],\n",
       "  },\n",
       "  (PV, PV_link, bus)={ edge_index=[2, 4] },\n",
       "  (bus, PV_link, PV)={ edge_index=[2, 4] },\n",
       "  (PQ, PQ_link, bus)={ edge_index=[2, 9] },\n",
       "  (bus, PQ_link, PQ)={ edge_index=[2, 9] },\n",
       "  (slack, slack_link, bus)={ edge_index=[2, 1] },\n",
       "  (bus, slack_link, slack)={ edge_index=[2, 1] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_14_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968caffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, data, hidden_size: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Linear projection for all node features\n",
    "        self.node_projections = nn.ModuleDict({\n",
    "            node_type: nn.Linear(data.num_node_features[node_type], hidden_size)\n",
    "            for node_type in data.num_node_features.keys()\n",
    "        })\n",
    "        # Linear projection for all edge features\n",
    "        self.edge_projections = nn.ModuleDict({\n",
    "            str(edge_type): nn.Linear(data.num_edge_features[edge_type], hidden_size)\n",
    "            for edge_type in data.num_edge_features.keys() if data.num_edge_features[edge_type] != 0\n",
    "            # not including subnode links which have no attributes.\n",
    "        })\n",
    "\n",
    "    def forward(self, data):\n",
    "        device = data[\"x\"].device\n",
    "        projected_nodes = {\n",
    "            node_type: self.node_projections[node_type](data[node_type].x)\n",
    "            for node_type in data.num_node_features.keys()\n",
    "        }\n",
    "\n",
    "        projected_edges = {}\n",
    "        for edge_type in data.edge_types:\n",
    "            if \"edge_attr\" in data[edge_type]:\n",
    "                projected_edges[str(edge_type)] = self.edge_projections[str(edge_type)](data[edge_type].edge_attr)\n",
    "            elif edge_type[2] != \"bus\":\n",
    "                num_edges = data[edge_type]['edge_index'].shape[1]\n",
    "                projected_edges[str(edge_type)] = torch.zeros((num_edges, self.hidden_size), device=device)\n",
    "\n",
    "        return projected_nodes, projected_edges\n",
    "\n",
    "\n",
    "# Interaction Network Module\n",
    "class InteractionNetwork(nn.Module):\n",
    "    def __init__(self, edge_type_dict, node_type_dict, edge_dim, node_dim, hidden_dim, include_sent_messages=False):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of the Interaction Network.\n",
    "        Args:\n",
    "            projected_edges (dict): Dictionary of projected edge features.\n",
    "            projected_nodes (dict): Dictionary of projected node features.\n",
    "            edge_dim (int): Dimension of edge features.\n",
    "            node_dim (int): Dimension of node features.\n",
    "            hidden_dim (int): Hidden layer size.\n",
    "            include_sent_messages (bool): Whether to include messages from sender edges in node update.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.include_sent_messages = include_sent_messages\n",
    "        self.edge_update = EdgeUpdate(edge_dim, node_dim, hidden_dim, edge_type_dict)\n",
    "        self.node_update = NodeUpdate(node_dim, hidden_dim, node_type_dict, self.include_sent_messages)\n",
    "\n",
    "    def forward(self, nodes, edges, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the Interaction Network.\n",
    "        Args:\n",
    "            nodes (Dict): !!!!\n",
    "            edges (Dict): !!!!\n",
    "            senders (Tensor): Indices of sender nodes [num_edges].\n",
    "            receivers (Tensor): Indices of receiver nodes [num_edges].\n",
    "        Returns:\n",
    "            Updated nodes and edges.\n",
    "        \"\"\"\n",
    "        # Apply marshalling and relational model phi_r (edge update)\n",
    "        # phi_r is applied onto src node features, dst node features, and edges\n",
    "        device = data[\"x\"].device\n",
    "        edge_hidden_dim = (edges.get(\"('bus', 'ac_line', 'bus'\") or edges.get(\"('bus', 'branch', 'bus'\")).shape[-1]\n",
    "        sent_received_node_type = {node_type: torch.zeros(n.shape[0], edge_hidden_dim, device=device) for node_type, n in nodes.items()}\n",
    "        updated_nodes_dict = {}\n",
    "        updated_edges_dict = {}\n",
    "\n",
    "        for edge_type, edge_feats in edges.items():\n",
    "            edge_type_tuple = tuple(edge_type.strip(\"()\").replace(\"'\", \"\").split(\", \"))\n",
    "            sender_type, receiver_type = edge_type_tuple[0], edge_type_tuple[2]\n",
    "            if sender_type != \"bus\":\n",
    "                continue\n",
    "            senders, receivers = data[edge_type_tuple].edge_index\n",
    "\n",
    "            # Gather node features\n",
    "            sender_features = nodes[sender_type][senders]\n",
    "            receiver_features = nodes[receiver_type][receivers]\n",
    "\n",
    "            # Calculate edge updates\n",
    "            updated_edges = self.edge_update(edge_feats, sender_features, receiver_features, edge_type)\n",
    "\n",
    "            # Pass messages\n",
    "            sent_received_node_type[receiver_type].scatter_add_(0, receivers.unsqueeze(-1).expand_as(updated_edges), updated_edges)\n",
    "            if self.include_sent_messages:\n",
    "                sent_received_node_type[sender_type].scatter_add_(0, senders.unsqueeze(-1).expand_as(updated_edges), updated_edges)\n",
    "\n",
    "            updated_edges_dict[edge_type] = updated_edges + edge_feats\n",
    "\n",
    "        # Apply the object model phi_o (node_update)\n",
    "        # phi_o is applied to the aggregated edge features (with sent and recieved messages)\n",
    "        for node_type, node_feats in nodes.items():\n",
    "            updated_nodes = self.node_update(node_feats, sent_received_node_type[node_type], node_type)\n",
    "            updated_nodes_dict[node_type] = updated_nodes + node_feats\n",
    "\n",
    "        return updated_nodes_dict, updated_edges_dict\n",
    "\n",
    "\n",
    "# Relational and Object models (phi_r and phi_o)\n",
    "class EdgeUpdate(nn.Module):\n",
    "    def __init__(self, edge_dim, node_dim, hidden_dim, edge_type_dict):\n",
    "        \"\"\"\n",
    "        Edge update function for updating edge features.\n",
    "        Args:\n",
    "            edge_dim (int): Dimension of edge features.\n",
    "            node_dim (int): Dimension of node features.\n",
    "            hidden_dim (int): Hidden layer size.\n",
    "            out_dim (int): Output edge feature dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleDict({edge_type: nn.Sequential(\n",
    "            nn.Linear(edge_dim + 2 * node_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, edge_dim)\n",
    "        ) for edge_type in edge_type_dict.keys()})\n",
    "\n",
    "    def forward(self, edges, sender_features, receiver_features, edge_type):\n",
    "        \"\"\"\n",
    "        Compute updated edge features.\n",
    "        Args:\n",
    "            edges (Tensor): Shape [num_edges, edge_feat_dim].\n",
    "            sender_features (Tensor): Shape [num_edges, node_feat_dim].\n",
    "            receiver_features (Tensor): Shape [num_edges, node_feat_dim].\n",
    "        Returns:\n",
    "            Tensor: Updated edge features of shape [num_edges, out_dim].\n",
    "        \"\"\"\n",
    "        x = torch.cat([edges, sender_features, receiver_features], dim=-1)\n",
    "        return self.mlps[edge_type](x)\n",
    "\n",
    "\n",
    "class NodeUpdate(nn.Module):\n",
    "    def __init__(self, node_dim, hidden_dim, node_type_dict, include_sent_messages=False):\n",
    "        \"\"\"\n",
    "        Node update module for updating node features.\n",
    "        Args:\n",
    "            input_dim (int): Dimension of node features.\n",
    "            output_dim (int): Output node feature dimension.\n",
    "            include_sent_messages (bool): Whether to include messages from sender edges\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.include_sent_messages = include_sent_messages\n",
    "        self.mlps = nn.ModuleDict({node_type: nn.Sequential(\n",
    "            nn.Linear(node_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, node_dim)\n",
    "        ) for node_type in node_type_dict.keys()})\n",
    "\n",
    "    def forward(self, node_features, updated_messages, node_type):\n",
    "        \"\"\"\n",
    "        Compute updated node features.\n",
    "        Args:\n",
    "            node_features (Tensor): Shape [num_nodes, node_feat_dim].\n",
    "            received_messages (Tensor): Shape [num_nodes, node_feat_dim].\n",
    "            sent_messages (Tensor, optional): Shape [num_nodes, node_feat_dim].\n",
    "        Returns:\n",
    "            Tensor: Updated node features of shape [num_nodes, output_dim].\n",
    "        \"\"\"\n",
    "        x = torch.cat([node_features, updated_messages], dim=-1)\n",
    "        return self.mlps[node_type](x)\n",
    "\n",
    "#  Decoders \n",
    "class DecoderOPF(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(DecoderOPF, self).__init__()\n",
    "\n",
    "        # Linear projection for all node features\n",
    "        self.node_decodings = nn.ModuleDict({\n",
    "            node_type: nn.Sequential(nn.Linear(hidden_size, 256),\n",
    "                                     nn.LayerNorm(256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, 256),\n",
    "                                     nn.LayerNorm(256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, 2)\n",
    "                                     )\n",
    "            for node_type in [\"bus\", \"generator\"]\n",
    "        })\n",
    "\n",
    "    def forward(self, node_dict, data):\n",
    "        # pmin, pmax = data[\"generator\"].x[:, 2:4].T\n",
    "        # qmin, qmax = data[\"generator\"].x[:, 5:7].T\n",
    "        # vmin, vmax = data[\"bus\"].x[:, 2:].T\n",
    "        pmin, pmax = data[\"generator\"][\"p_lims\"].T\n",
    "        qmin, qmax = data[\"generator\"][\"q_lims\"].T\n",
    "        vmin, vmax = data[\"bus\"][\"v_lims\"].T\n",
    "\n",
    "        output_nodes = {\n",
    "            node_type: self.node_decodings[node_type](node_dict[node_type])\n",
    "            for node_type in [\"bus\", \"generator\"]\n",
    "        }\n",
    "\n",
    "        # Passing vm, pg, qg through the sigmoid layer.\n",
    "        output_va = output_nodes[\"bus\"][:, 0]\n",
    "        output_vm = torch.sigmoid(output_nodes[\"bus\"][:, -1]) * (vmax - vmin) + vmin\n",
    "        output_pg = torch.sigmoid(output_nodes[\"generator\"][:, 0]) * (pmax - pmin) + pmin\n",
    "        output_qg = torch.sigmoid(output_nodes[\"generator\"][:, -1]) * (qmax - qmin) + qmin\n",
    "\n",
    "        output_dict = {\n",
    "            \"bus\": torch.stack([output_va, output_vm], dim=1),\n",
    "            \"generator\": torch.stack([output_pg, output_qg], dim=1)\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "class DecoderPF(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(DecoderPF, self).__init__()\n",
    "\n",
    "        # Linear projection for all node features\n",
    "        self.node_decodings = nn.ModuleDict({\n",
    "            node_type: nn.Sequential(nn.Linear(hidden_size, 256),\n",
    "                                     nn.LayerNorm(256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, 256),\n",
    "                                     nn.LayerNorm(256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, 2)\n",
    "                                     )\n",
    "            for node_type in [\"PV\", \"PQ\", \"slack\"]\n",
    "        })\n",
    "\n",
    "    def forward(self, node_dict, data):\n",
    "        \n",
    "        device = data[\"x\"].device\n",
    "\n",
    "        output_dict = {\n",
    "            node_type: self.node_decodings[node_type](node_dict[node_type])\n",
    "            for node_type in [\"PV\", \"PQ\", \"slack\"]\n",
    "        }\n",
    "\n",
    "        # Reconstructing the bus-level data\n",
    "        num_buses = data[\"bus\"].num_nodes\n",
    "        bus_va = torch.zeros(num_buses, device=device)\n",
    "        bus_vm = torch.zeros(num_buses, device=device)\n",
    "\n",
    "        # PQ\n",
    "        pq_idx = data[\"PQ\", \"PQ_link\", \"bus\"].edge_index[1]\n",
    "        pq_outputs = output_dict[\"PQ\"]\n",
    "        bus_va[pq_idx] = pq_outputs[:, 0]\n",
    "        bus_vm[pq_idx] = pq_outputs[:, 1]\n",
    "\n",
    "        # PV\n",
    "        pv_idx = data[\"PV\", \"PV_link\", \"bus\"].edge_index[1]\n",
    "        pv_outputs = output_dict[\"PV\"]\n",
    "        bus_va[pv_idx] = pv_outputs[:, 1]\n",
    "        bus_vm[pv_idx] = data[\"PV\"].x[:, 1]\n",
    "\n",
    "        # Slack\n",
    "        slack_idx = data[\"slack\", \"slack_link\", \"bus\"].edge_index[1]\n",
    "        slack_va_vm = data[\"slack\"].x\n",
    "        bus_va[slack_idx] = slack_va_vm[:, 0]\n",
    "        bus_vm[slack_idx] = slack_va_vm[:, 1]\n",
    "\n",
    "        output_dict[\"bus\"] = torch.stack([bus_va, bus_vm], dim=-1)\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ca61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANOS_PF(nn.Module):\n",
    "    def __init__(self, dataset, hidden_dim, include_sent_messages, k_steps):\n",
    "        super().__init__()\n",
    "        edge_feat_dim = node_feat_dim = hidden_dim\n",
    "\n",
    "        # Define the encoder to get projected nodes and edges\n",
    "        self.encoder = Encoder(data=dataset, hidden_size=hidden_dim)\n",
    "\n",
    "        # Interaction network layers for message passing\n",
    "        node_type_dict = {\n",
    "            node_type: True\n",
    "            for node_type in dataset[0].num_node_features.keys()\n",
    "        }\n",
    "\n",
    "        edge_type_dict = {\n",
    "            str(edge_type): True if \"edge_attr\" in dataset[0][edge_type] else False\n",
    "            for edge_type in dataset[0].edge_types\n",
    "            if \"bus\" in edge_type[0]  # Only include edges where \"bus\" is the source\n",
    "        }\n",
    "\n",
    "        self.message_passing_layers = nn.ModuleList(\n",
    "            InteractionNetwork(edge_type_dict=edge_type_dict,\n",
    "                               node_type_dict=node_type_dict,\n",
    "                               edge_dim=edge_feat_dim,\n",
    "                               node_dim=node_feat_dim,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               include_sent_messages=include_sent_messages) for _ in range(k_steps))\n",
    "\n",
    "        # Define the decoder to get the model outputs\n",
    "        self.decoder = DecoderPF(hidden_size=hidden_dim)\n",
    "        self.k_steps = k_steps\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        # Encoding\n",
    "        projected_nodes, projected_edges = self.encoder(data)\n",
    "\n",
    "        # Message passing layers with residual connections\n",
    "        nodes, edges = projected_nodes, projected_edges\n",
    "        for l in range(self.k_steps):\n",
    "            nodes, edges = self.message_passing_layers[l](nodes, edges, data)\n",
    "\n",
    "        # Decoding\n",
    "        output_dict = self.decoder(nodes, data)\n",
    "\n",
    "        # Deriving branch flows\n",
    "        p_fr, q_fr, p_to, q_to = self.derive_branch_flows(output_dict, data)\n",
    "        output_dict[\"edge_preds\"] = torch.stack([p_to, q_to, p_fr, q_fr], dim=-1) \n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def derive_branch_flows(self, output_dict, data):\n",
    "\n",
    "        # Create complex voltage\n",
    "        va = output_dict[\"bus\"][:, 0]\n",
    "        vm = output_dict[\"bus\"][:, -1]\n",
    "        v_complex = vm * torch.exp(1j* va)\n",
    "\n",
    "        # Extract edge info\n",
    "        edge_index = data[\"bus\", \"branch\", \"bus\"].edge_index\n",
    "        edge_attr = data[\"bus\", \"branch\", \"bus\"].edge_attr\n",
    "\n",
    "        # Unpack edge features\n",
    "        br_r, br_x = edge_attr[:, 0], edge_attr[:, 1]\n",
    "        b_fr, b_to = edge_attr[:, 3], edge_attr[:, 5]\n",
    "        g_fr, g_to = edge_attr[:, 2], edge_attr[:, 4]\n",
    "        tap = edge_attr[:, 6]\n",
    "        shift = edge_attr[:, 7]\n",
    "\n",
    "        # Complex tap ratio\n",
    "        T_complex = tap * torch.exp(1j * shift)\n",
    "\n",
    "        # Complex admittances\n",
    "        Y_branch = 1 / (br_r + 1j * br_x)\n",
    "        Y_c_fr = g_fr + 1j * b_fr\n",
    "        Y_c_to = g_to + 1j * b_to\n",
    "\n",
    "        # Node voltages\n",
    "        i, j = edge_index[0], edge_index[1]\n",
    "        vi = v_complex[i]\n",
    "        vj = v_complex[j]\n",
    "\n",
    "        # Sending-end complex power\n",
    "        S_fr = ((Y_branch + Y_c_fr).conj() * (vi.abs() ** 2)) / (T_complex.abs() ** 2) - \\\n",
    "            (Y_branch.conj() * (vi * vj.conj())) / T_complex\n",
    "\n",
    "        # Receiving-end complex power\n",
    "        S_to = (Y_branch + Y_c_to).conj() * (vj.abs() ** 2) - \\\n",
    "            Y_branch.conj() * (vj * vi.conj()) / T_complex.conj()\n",
    "\n",
    "        # Real/reactive power flows\n",
    "        p_fr, q_fr = S_fr.real, S_fr.imag\n",
    "        p_to, q_to = S_to.real, S_to.imag\n",
    "\n",
    "        return p_fr, q_fr, p_to, q_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d98c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class constraint_violations_loss_pf:\n",
    "    def __init__(self, ):\n",
    "        self.constraint_loss = None\n",
    "        self.bus_real_mismatch = None\n",
    "        self.bus_reactive_mismatch = None\n",
    "        \n",
    "    def __call__(self, output_dict, data):\n",
    "\n",
    "        device = data[\"x\"].device\n",
    "\n",
    "        # Get the predictions and edge features\n",
    "        bus_pred = output_dict[\"bus\"]\n",
    "        edge_pred = output_dict[\"edge_preds\"]\n",
    "        edge_indices = data[\"bus\", \"branch\", \"bus\"].edge_index\n",
    "        edge_features = data[\"bus\", \"branch\", \"bus\"].edge_attr\n",
    "        va, vm = bus_pred.T\n",
    "        complex_voltage = vm * torch.exp(1j* va)\n",
    "\n",
    "        # Get the branch flows from the edge predictions\n",
    "        n = data[\"bus\"].x.shape[0]\n",
    "        sum_branch_flows = torch.zeros(n, dtype=torch.cfloat, device=device)\n",
    "        flows_rev = edge_pred[:, 0] + 1j * edge_pred[:, 1]  \n",
    "        flows_fwd = edge_pred[:, 2] + 1j * edge_pred[:, 3]  \n",
    "        sum_branch_flows.scatter_add_(0, edge_indices[0], flows_fwd)\n",
    "        sum_branch_flows.scatter_add_(0, edge_indices[1], flows_rev)\n",
    "\n",
    "        # Generator flows (already aggregated per bus)\n",
    "        bus_gen = data[\"bus\"].bus_gen.to(device) \n",
    "        gen_flows = bus_gen[:, 0] + 1j * bus_gen[:, 1]\n",
    "\n",
    "        # Demand flows (already aggregated per bus)\n",
    "        bus_demand = data[\"bus\"].bus_demand.to(device) \n",
    "        demand_flows = bus_demand[:, 0] + 1j * bus_demand[:, 1]\n",
    "\n",
    "        # Shunt admittances\n",
    "        bus_shunts = data[\"bus\"].shunt.to(device)  \n",
    "        shunt_flows = (torch.abs(vm) ** 2) * (bus_shunts[:, 1] + 1j * bus_shunts[:, 0])  # (b_shunt + j*g_shunt)\n",
    "\n",
    "        power_balance = gen_flows - demand_flows - shunt_flows - sum_branch_flows\n",
    "        real_power_mismatch = torch.abs(torch.real(power_balance))\n",
    "        reactive_power_mismatch = torch.abs(torch.imag(power_balance))\n",
    "\n",
    "        # power: real and imaginary mismatches\n",
    "        violation_degree_real_mismatch = real_power_mismatch.mean()\n",
    "        violation_degree_imag_mismatch = reactive_power_mismatch.mean()\n",
    "\n",
    "        # branch flows: ground truth mismatch, real\n",
    "        p_flows_true = data[\"bus\", \"branch\", \"bus\"].edge_label[:,-2] # this is from bus flow\n",
    "        p_flows_mismatch = torch.real(flows_fwd) - p_flows_true\n",
    "        violation_degree_real_flow_mismatch = torch.abs(p_flows_mismatch).mean()\n",
    "\n",
    "        # branch flows: ground truth mismatch, reactive\n",
    "        q_flows_true = data[\"bus\", \"branch\", \"bus\"].edge_label[:,-1] # this is from bus flow\n",
    "        q_flows_mismatch = torch.imag(flows_fwd) - q_flows_true\n",
    "        violation_degree_imag_flow_mismatch = torch.abs(q_flows_mismatch).mean()\n",
    "\n",
    "        # loss\n",
    "        loss_c = (violation_degree_real_mismatch + violation_degree_imag_mismatch + \n",
    "                  violation_degree_real_flow_mismatch + violation_degree_imag_flow_mismatch)\n",
    "        \n",
    "\n",
    "        self.constraint_loss = loss_c\n",
    "        self.bus_real_mismatch = violation_degree_real_mismatch\n",
    "        self.bus_reactive_mismatch = violation_degree_imag_mismatch\n",
    "        self.real_flow_mismatch_violation = violation_degree_real_flow_mismatch\n",
    "        self.imag_flow_mismatch_violation = violation_degree_imag_flow_mismatch\n",
    "\n",
    "        return loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27216df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CANOSMSE(output_dict, data):\n",
    "    # Gather predictions\n",
    "    PV_pred, PQ_pred, slack_pred = output_dict[\"PV\"], output_dict[\"PQ\"], output_dict[\"slack\"]\n",
    "    edge_preds = output_dict[\"edge_preds\"]\n",
    "\n",
    "    # Gather targets\n",
    "    PV_target, PQ_target, slack_target = data[\"PV\"].y, data[\"PQ\"].y, data[\"slack\"].y\n",
    "    branch_target = data[\"bus\", \"branch\", \"bus\"].edge_label\n",
    "\n",
    "    # Calculate L2 loss\n",
    "    pv_loss = torch.nn.functional.mse_loss(PV_pred, PV_target)\n",
    "    pq_loss = torch.nn.functional.mse_loss(PQ_pred, PQ_target)\n",
    "    slack_loss = torch.nn.functional.mse_loss(slack_pred, slack_target)\n",
    "    edge_loss = torch.nn.functional.mse_loss(edge_preds, branch_target)\n",
    "\n",
    "    total_loss = pv_loss + pq_loss + slack_loss + edge_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907c1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SmallCanos = CANOS_PF(dataset=case_14_data,\n",
    "                    hidden_dim=128,\n",
    "                    include_sent_messages=True,\n",
    "                    k_steps=5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af4d1d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CANOS_PF(\n",
       "  (encoder): Encoder(\n",
       "    (node_projections): ModuleDict(\n",
       "      (bus): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (PQ): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (PV): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (slack): Linear(in_features=2, out_features=128, bias=True)\n",
       "    )\n",
       "    (edge_projections): ModuleDict(\n",
       "      (('bus', 'branch', 'bus')): Linear(in_features=8, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (message_passing_layers): ModuleList(\n",
       "    (0-4): 5 x InteractionNetwork(\n",
       "      (edge_update): EdgeUpdate(\n",
       "        (mlps): ModuleDict(\n",
       "          (('bus', 'branch', 'bus')): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (('bus', 'PV_link', 'PV')): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (('bus', 'PQ_link', 'PQ')): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (('bus', 'slack_link', 'slack')): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (node_update): NodeUpdate(\n",
       "        (mlps): ModuleDict(\n",
       "          (bus): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (PQ): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (PV): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (slack): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderPF(\n",
       "    (node_decodings): ModuleDict(\n",
       "      (PV): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "      (PQ): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "      (slack): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmallCanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe4b2d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NodeStorage' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss_mse \u001b[38;5;241m=\u001b[39m CANOSMSE(output_dict, batch)  \u001b[38;5;66;03m# MSE-based loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mCANOS_PF.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     projected_nodes, projected_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Message passing layers with residual connections\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     nodes, edges \u001b[38;5;241m=\u001b[39m projected_nodes, projected_edges\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m---> 19\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     20\u001b[0m     projected_nodes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m         node_type: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_projections[node_type](data[node_type]\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node_type \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mnum_node_features\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     23\u001b[0m     }\n\u001b[1;32m     25\u001b[0m     projected_edges \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/pfdelta/lib/python3.10/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NodeStorage' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCanos.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_loader = DataLoader(case_14_data, batch_size=4, shuffle=True)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i == 5:\n",
    "            pass  # Profiling placeholder\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output_dict = model(batch)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_mse = CANOSMSE(output_dict, batch)  # MSE-based loss\n",
    "        loss_constraints = constraint_violations_loss_pf(output_dict, batch)  # Constraint violation loss\n",
    "\n",
    "        # Total loss (weighted sum if needed)\n",
    "        loss = loss_mse + 0.1*loss_constraints\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
