{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b85888",
   "metadata": {},
   "source": [
    "# PFΔ Dataset Quick Start Guide\n",
    "This notebook demonstrates how to initialize and explore the PFDeltaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH = '/mnt/home/donti-group-shared/pfdelta_neurips'\n",
    "sys.path.append(PATH)\n",
    "\n",
    "# Import required libraries\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from core.datasets.pfdelta_dataset import PFDeltaDataset\n",
    "from core.datasets.dataset_utils import (\n",
    "    canos_pf_data_mean0_var1,\n",
    "    canos_pf_slack_mean0_var1,\n",
    ")\n",
    "from core.datasets.data_stats import canos_pfdelta_stats, pfnet_pfdata_stats\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f711e6c",
   "metadata": {},
   "source": [
    "### 1. Inititialize Dataset Class & Extract HuggingFace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee35c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files for task 1.3...\n",
      "Downloading shuffle files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://huggingface.co/datasets/pfdelta/pfdelta/resolve/main/shuffle_files.tar\n",
      "Extracting data/shuffle_files.tar\n",
      "Downloading https://huggingface.co/datasets/pfdelta/pfdelta/resolve/main/case14.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading case14 data from https://huggingface.co/datasets/pfdelta/pfdelta/resolve/main/case14.tar.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data/case14.tar.gz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted case14 data to data\n",
      "Processing combined data for task 1.3\n",
      "Processing split: CANOS 1.3 n train (16200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 16200/16200 [01:56<00:00, 139.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n val (1800 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 1800/1800 [00:13<00:00, 135.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:11<00:00, 174.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-1 train (16200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 16200/16200 [02:12<00:00, 122.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-1 val (1800 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 1800/1800 [00:14<00:00, 121.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-1 test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:17<00:00, 111.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-2 train (16200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 16200/16200 [02:31<00:00, 107.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-2 val (1800 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 1800/1800 [00:16<00:00, 110.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-2 test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:19<00:00, 102.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 153.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-1 near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 135.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.3 n-2 near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 114.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating combined train data with 48600 samples\n",
      "Saved combined train data with 48600 samples\n",
      "Collating combined val data with 5400 samples\n",
      "Saved combined val data with 5400 samples\n",
      "Collating combined test data with 6600 samples\n",
      "Saved combined test data with 6600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from data/processed/combined_task_1.3_CANOS_case14/train.pt\n",
      "Dataset initialized successfully!\n",
      "Number of samples: 48600\n",
      "Task: 1.3\n",
      "Case: case14\n",
      "Split: train\n"
     ]
    }
   ],
   "source": [
    "# Basic initialization for Task 1.3 (baseline task with all contingencies)\n",
    "dataset = PFDeltaDataset(\n",
    "    root_dir=\"data\",              # Root directory for dataset storage\n",
    "    case_name=\"case14\",           # Power system case (14-bus system)\n",
    "    perturbation=\"n\",             # Grid contingency: \"n\", \"n-1\", or \"n-2\"\n",
    "    feasibility_type=\"feasible\",  # \"feasible\", \"near infeasible\", or \"approaching infeasible\"\n",
    "    n_samples=-1,                 # -1 loads all available samples\n",
    "    split=\"train\",                # \"train\", \"val\", \"test\", or \"all\"\n",
    "    model=\"CANOS\",                  # Model identifier for processed file naming\n",
    "    task=1.3,                     # Benchmark task (1.1-4.3) or \"analysis\"\n",
    "    add_bus_type=False,           # Include bus-type-specific node sets\n",
    "    force_reload=False            # Force reprocessing of data\n",
    ")\n",
    "\n",
    "print(f\"Dataset initialized successfully!\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Task: {dataset.task}\")\n",
    "print(f\"Case: {dataset.case_name}\")\n",
    "print(f\"Split: {dataset.split}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ff978",
   "metadata": {},
   "source": [
    "### 2. Access and Print a Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c239a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  bus={\n",
       "    x=[14, 2],\n",
       "    y=[14, 2],\n",
       "    bus_gen=[14, 2],\n",
       "    bus_demand=[14, 2],\n",
       "    bus_voltages=[14, 2],\n",
       "    bus_type=[14],\n",
       "    shunt=[14, 2],\n",
       "    limits=[14, 2],\n",
       "  },\n",
       "  gen={\n",
       "    limits=[5, 4],\n",
       "    generation=[5, 2],\n",
       "    slack_gen=[5],\n",
       "  },\n",
       "  load={ demand=[11, 2] },\n",
       "  (bus, branch, bus)={\n",
       "    edge_index=[2, 20],\n",
       "    edge_attr=[20, 8],\n",
       "    edge_label=[20, 4],\n",
       "    edge_limits=[20, 1],\n",
       "  },\n",
       "  (gen, gen_link, bus)={ edge_index=[2, 5] },\n",
       "  (bus, gen_link, gen)={ edge_index=[2, 5] },\n",
       "  (load, load_link, bus)={ edge_index=[2, 11] },\n",
       "  (bus, load_link, load)={ edge_index=[2, 11] }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764dc1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE DATA STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "Data type: <class 'torch_geometric.data.hetero_data.HeteroData'>\n",
      "\n",
      "Node types: ['bus', 'gen', 'load']\n",
      "Edge types: [('bus', 'branch', 'bus'), ('gen', 'gen_link', 'bus'), ('bus', 'gen_link', 'gen'), ('load', 'load_link', 'bus'), ('bus', 'load_link', 'load')]\n",
      "\n",
      "--- Node Features ---\n",
      "bus: torch.Size([14, 2])\n",
      "\n",
      "--- Edge Information ---\n",
      "('bus', 'branch', 'bus'): torch.Size([2, 20]) edges\n",
      "  Edge attributes: torch.Size([20, 8])\n",
      "('gen', 'gen_link', 'bus'): torch.Size([2, 5]) edges\n",
      "('bus', 'gen_link', 'gen'): torch.Size([2, 5]) edges\n",
      "('load', 'load_link', 'bus'): torch.Size([2, 11]) edges\n",
      "('bus', 'load_link', 'load'): torch.Size([2, 11]) edges\n",
      "\n",
      "--- Available Attributes ---\n",
      "- y\n",
      "- limits\n",
      "- bus_gen\n",
      "- edge_index\n",
      "- edge_limits\n",
      "- bus_demand\n",
      "- bus_type\n",
      "- edge_label\n",
      "- generation\n",
      "- slack_gen\n",
      "- shunt\n",
      "- demand\n",
      "- x\n",
      "- bus_voltages\n",
      "- edge_attr\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print sample type\n",
    "print(f\"\\nData type: {type(sample)}\")\n",
    "\n",
    "# If it's a HeteroData object, print its structure\n",
    "if hasattr(sample, 'node_types'):\n",
    "    print(f\"\\nNode types: {sample.node_types}\")\n",
    "    print(f\"Edge types: {sample.edge_types}\")\n",
    "    \n",
    "    # Print node features for each node type\n",
    "    print(\"\\n--- Node Features ---\")\n",
    "    for node_type in sample.node_types:\n",
    "        if hasattr(sample[node_type], 'x'):\n",
    "            print(f\"{node_type}: {sample[node_type].x.shape}\")\n",
    "    \n",
    "    # Print edge information\n",
    "    print(\"\\n--- Edge Information ---\")\n",
    "    for edge_type in sample.edge_types:\n",
    "        edge_index = sample[edge_type].edge_index\n",
    "        print(f\"{edge_type}: {edge_index.shape} edges\")\n",
    "        if hasattr(sample[edge_type], 'edge_attr'):\n",
    "            print(f\"  Edge attributes: {sample[edge_type].edge_attr.shape}\")\n",
    "\n",
    "# Print available attributes\n",
    "print(\"\\n--- Available Attributes ---\")\n",
    "if hasattr(sample, 'keys'):\n",
    "    for key in sample.keys():\n",
    "        print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831347a",
   "metadata": {},
   "source": [
    "### 3. Custom PFDeltaDataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffcab6",
   "metadata": {},
   "source": [
    "PFDeltaDataset can be inherited and tailored for model-specific data preprocessing. To create a custom dataset variant, you can:\n",
    "\n",
    "- Override __init__ to add custom initialization logic (e.g., model-specific transforms or normalization)\n",
    "\n",
    "- Override build_heterodata to implement custom graph construction or preprocessing steps\n",
    "\n",
    "Below is a simplified example for the CANOS architecture. Note that the actual CANOS implementation includes additional normalization transforms using case-specific statistics—this example focuses on the core graph pruning logic for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ac1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFDeltaCANOS(PFDeltaDataset):\n",
    "    \"\"\"\n",
    "    Simplified PFDelta dataset variant for CANOS model.\n",
    "    \n",
    "    Prunes the heterogeneous graph to include only the node types\n",
    "    required by CANOS: bus, PV, PQ, and slack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"data\",\n",
    "        case_name=\"\",\n",
    "        split=\"train\",\n",
    "        model=\"CANOS\",\n",
    "        task=1.1,\n",
    "        add_bus_type=True,\n",
    "        force_reload=False,\n",
    "    ):\n",
    "        # Initialize parent class with CANOS defaults\n",
    "        super().__init__(\n",
    "            root_dir=root_dir,\n",
    "            case_name=case_name,\n",
    "            split=split,\n",
    "            model=model,\n",
    "            task=task,\n",
    "            add_bus_type=add_bus_type,\n",
    "            force_reload=force_reload,\n",
    "        )\n",
    "\n",
    "    def build_heterodata(self, pm_case: dict, is_cpf_sample: bool = False):\n",
    "        \"\"\"\n",
    "        Build a CANOS-compatible HeteroData graph with pruned node types.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pm_case : dict\n",
    "            PowerModels.jl case dictionary with bus, branch, gen, and load data\n",
    "        is_cpf_sample : bool\n",
    "            Whether this is a continuation power flow sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        data : HeteroData\n",
    "            Processed graph with only bus, PV, PQ, and slack nodes\n",
    "        \"\"\"\n",
    "        # Build the full heterogeneous graph using parent method\n",
    "        data = super().build_heterodata(pm_case, is_cpf_sample=is_cpf_sample)\n",
    "\n",
    "        # Prune to keep only CANOS-required node types\n",
    "        keep_nodes = {\"bus\", \"PV\", \"PQ\", \"slack\"}\n",
    "\n",
    "        # Remove unwanted node types\n",
    "        for node_type in list(data.node_types):\n",
    "            if node_type not in keep_nodes:\n",
    "                del data[node_type]\n",
    "\n",
    "        # Remove edges connected to deleted node types\n",
    "        for edge_type in list(data.edge_types):\n",
    "            src, _, dst = edge_type\n",
    "            if src not in keep_nodes or dst not in keep_nodes:\n",
    "                del data[edge_type]\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0950f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files for task 1.1...\n",
      "Shuffle files already exist. Skipping download.\n",
      "case14 data already exists. Skipping download.\n",
      "Processing combined data for task 1.1\n",
      "Processing split: CANOS 1.1 n train (48600 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 100%|██████████| 48600/48600 [06:54<00:00, 117.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n val (5400 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 100%|██████████| 5400/5400 [00:50<00:00, 107.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:14<00:00, 134.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-1 train (0 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-1 val (0 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-1 test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:21<00:00, 94.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-2 train (0 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train data: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-2 val (0 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building val data: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-2 test (2000 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 2000/2000 [00:13<00:00, 153.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 144.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-1 near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 121.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: CANOS 1.1 n-2 near infeasible test (200 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 200/200 [00:01<00:00, 160.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating combined train data with 48600 samples\n",
      "Saved combined train data with 48600 samples\n",
      "Collating combined val data with 5400 samples\n",
      "Saved combined val data with 5400 samples\n",
      "Collating combined test data with 6600 samples\n",
      "Saved combined test data with 6600 samples\n",
      "Loading train dataset from data/processed/combined_task_1.1_CANOS_case14/train.pt\n",
      "CANOS Dataset initialized successfully!\n",
      "Number of samples: 48600\n",
      "Task: 1.1\n",
      "Case: case14\n",
      "Split: train\n",
      "\n",
      "HeteroData Structure (CANOS-pruned):\n",
      "Node types: ['bus', 'PQ', 'PV', 'slack']\n",
      "Edge types: [('bus', 'branch', 'bus'), ('PV', 'PV_link', 'bus'), ('bus', 'PV_link', 'PV'), ('PQ', 'PQ_link', 'bus'), ('bus', 'PQ_link', 'PQ'), ('slack', 'slack_link', 'bus'), ('bus', 'slack_link', 'slack')]\n",
      "\n",
      "--- Node Information ---\n",
      "  bus       :  14 nodes,   2 features\n",
      "  PQ        :   9 nodes,   2 features\n",
      "  PV        :   4 nodes,   2 features\n",
      "  slack     :   1 nodes,   2 features\n",
      "\n",
      "--- Edge Connectivity ---\n",
      "  bus        --(branch)--> bus       : 20 edges\n",
      "  PV         --(PV_link)--> bus       : 4 edges\n",
      "  bus        --(PV_link)--> PV        : 4 edges\n",
      "  PQ         --(PQ_link)--> bus       : 9 edges\n",
      "  bus        --(PQ_link)--> PQ        : 9 edges\n",
      "  slack      --(slack_link)--> bus       : 1 edges\n",
      "  bus        --(slack_link)--> slack     : 1 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialize CANOS dataset for Task 1.1\n",
    "dataset_canos = PFDeltaCANOS(\n",
    "    root_dir=\"data\",              # Root directory for dataset storage\n",
    "    case_name=\"case14\",           # Power system case (14-bus system)\n",
    "    split=\"train\",                # \"train\", \"val\", or \"test\"\n",
    "    model=\"CANOS\",                # Model identifier\n",
    "    task=1.1,                     # Benchmark task\n",
    "    add_bus_type=True,            # Include bus type encodings\n",
    "    force_reload=False            # Force reprocessing of data\n",
    ")\n",
    "\n",
    "print(f\"CANOS Dataset initialized successfully!\")\n",
    "print(f\"Number of samples: {len(dataset_canos)}\")\n",
    "print(f\"Task: {dataset_canos.task}\")\n",
    "print(f\"Case: {dataset_canos.case_name}\")\n",
    "print(f\"Split: {dataset_canos.split}\")\n",
    "\n",
    "# Get a sample and inspect the pruned heterograph\n",
    "sample = dataset_canos[0]\n",
    "\n",
    "print(f\"\\nHeteroData Structure (CANOS-pruned):\")\n",
    "print(f\"Node types: {sample.node_types}\")\n",
    "print(f\"Edge types: {sample.edge_types}\")\n",
    "\n",
    "# Show node counts and features\n",
    "print(\"\\n--- Node Information ---\")\n",
    "for node_type in sample.node_types:\n",
    "    if hasattr(sample[node_type], 'x'):\n",
    "        num_nodes = sample[node_type].x.shape[0]\n",
    "        num_features = sample[node_type].x.shape[1]\n",
    "        print(f\"  {node_type:10s}: {num_nodes:3d} nodes, {num_features:3d} features\")\n",
    "\n",
    "# Show edge connections\n",
    "print(\"\\n--- Edge Connectivity ---\")\n",
    "for edge_type in sample.edge_types:\n",
    "    src, relation, dst = edge_type\n",
    "    num_edges = sample[edge_type].edge_index.shape[1]\n",
    "    print(f\"  {src:10s} --({relation})--> {dst:10s}: {num_edges} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c183a8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  bus={\n",
       "    x=[14, 2],\n",
       "    y=[14, 2],\n",
       "    bus_gen=[14, 2],\n",
       "    bus_demand=[14, 2],\n",
       "    bus_voltages=[14, 2],\n",
       "    bus_type=[14],\n",
       "    shunt=[14, 2],\n",
       "    limits=[14, 2],\n",
       "  },\n",
       "  PQ={\n",
       "    x=[9, 2],\n",
       "    y=[9, 2],\n",
       "  },\n",
       "  PV={\n",
       "    x=[4, 2],\n",
       "    y=[4, 2],\n",
       "    generation=[4, 2],\n",
       "    demand=[4, 2],\n",
       "  },\n",
       "  slack={\n",
       "    x=[1, 2],\n",
       "    y=[1, 2],\n",
       "    generation=[1, 2],\n",
       "    demand=[1, 2],\n",
       "  },\n",
       "  (bus, branch, bus)={\n",
       "    edge_index=[2, 20],\n",
       "    edge_attr=[20, 8],\n",
       "    edge_label=[20, 4],\n",
       "    edge_limits=[20, 1],\n",
       "  },\n",
       "  (PV, PV_link, bus)={ edge_index=[2, 4] },\n",
       "  (bus, PV_link, PV)={ edge_index=[2, 4] },\n",
       "  (PQ, PQ_link, bus)={ edge_index=[2, 9] },\n",
       "  (bus, PQ_link, PQ)={ edge_index=[2, 9] },\n",
       "  (slack, slack_link, bus)={ edge_index=[2, 1] },\n",
       "  (bus, slack_link, slack)={ edge_index=[2, 1] }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cc7c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfdelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
